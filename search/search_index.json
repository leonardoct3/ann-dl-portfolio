{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Neural Networks &amp; Deep Learning Portfolio","text":"Academic Term <p>2025.1</p>"},{"location":"#author","title":"Author","text":"<p>Leonardo Teixeira Artificial Neural Networks and Deep Learning Course</p>"},{"location":"#portfolio-overview","title":"Portfolio Overview","text":"<p>This portfolio documents my journey through the Artificial Neural Networks and Deep Learning course, showcasing both theoretical understanding and practical implementation of neural network concepts.</p> <p>Portfolio Structure</p> <p>This portfolio is organized into Exercises and Projects. Each section demonstrates different aspects of neural network design, implementation, and analysis. Navigate through the sidebar to explore specific topics.</p>"},{"location":"#progress-tracker","title":"Progress Tracker","text":""},{"location":"#exercises","title":"Exercises","text":"<ul> <li> Exercise 1 - Data Analysis \u2705 Completed</li> <li>2D Class Separability Analysis</li> <li>Higher-Dimensional Non-linearity  </li> <li>Real-World Data Preprocessing</li> <li> Exercise 2 - Neural Network Fundamentals</li> <li> Exercise 3 - Training Optimization</li> <li> Exercise 4 - Advanced Architectures</li> </ul>"},{"location":"#projects","title":"Projects","text":"<ul> <li> Final Project - To be announced</li> </ul>"},{"location":"#learning-objectives","title":"Learning Objectives","text":"<p>Throughout this course, I aim to develop expertise in:</p> <ol> <li>Data Analysis &amp; Preprocessing</li> <li>Understanding class separability</li> <li>Feature engineering for neural networks</li> <li> <p>Handling real-world datasets</p> </li> <li> <p>Neural Network Fundamentals</p> </li> <li>Perceptrons vs Multi-Layer Perceptrons</li> <li>Activation functions and their properties</li> <li> <p>Forward and backward propagation</p> </li> <li> <p>Training &amp; Optimization</p> </li> <li>Loss functions and optimization algorithms</li> <li>Regularization techniques</li> <li> <p>Hyperparameter tuning</p> </li> <li> <p>Advanced Architectures</p> </li> <li>Convolutional Neural Networks (CNNs)</li> <li>Recurrent Neural Networks (RNNs)</li> <li>Modern architectures and techniques</li> </ol>"},{"location":"#technical-skills-demonstrated","title":"Technical Skills Demonstrated","text":""},{"location":"#programming-tools","title":"Programming &amp; Tools","text":"<ul> <li>Python: NumPy, Pandas, Matplotlib, Seaborn</li> <li>Machine Learning: Scikit-learn, TensorFlow/PyTorch</li> <li>Data Visualization: Statistical plots, PCA analysis</li> <li>Documentation: Markdown, MkDocs, Jupyter Notebooks</li> </ul>"},{"location":"#analytical-skills","title":"Analytical Skills","text":"<ul> <li>Statistical data analysis</li> <li>Dimensionality reduction techniques</li> <li>Class separability assessment</li> <li>Feature engineering strategies</li> </ul>"},{"location":"#portfolio-highlights","title":"Portfolio Highlights","text":""},{"location":"#exercise-1-data-analysis-mastery","title":"Exercise 1: Data Analysis Mastery","text":"<ul> <li>\u2705 Generated and analyzed 2D multi-class datasets</li> <li>\u2705 Explored linear vs nonlinear separability concepts</li> <li>\u2705 Applied PCA to 5D datasets for visualization</li> <li>\u2705 Preprocessed real-world Spaceship Titanic dataset</li> <li>\u2705 Optimized data for tanh activation functions</li> </ul> <p>Key Achievement: Demonstrated comprehensive understanding of how data characteristics influence neural network design decisions.</p>"},{"location":"#navigation-guide","title":"Navigation Guide","text":"<p>Use the sidebar to explore:</p> <ul> <li>Exercises: Detailed implementations and analyses</li> <li>Projects: Comprehensive applications of learned concepts  </li> <li>Documentation: Technical details about this portfolio</li> </ul> <p>Each exercise includes: - Problem statements and objectives - Step-by-step implementations - Visualizations and results - Key insights and learnings - Code examples with explanations</p>"},{"location":"#contact-collaboration","title":"Contact &amp; Collaboration","text":"<p>This portfolio represents my academic work in neural networks and deep learning. Feel free to explore the implementations and analyses documented here.</p> <p>GitHub Repository: ann-dl-portfolio</p>"},{"location":"#references","title":"References","text":"<p>Material for MkDocs</p>"},{"location":"exercises/data/main/","title":"Exercise 1 - Data Analysis","text":""},{"location":"exercises/data/main/#overview","title":"Overview","text":"<p>This exercise explores fundamental concepts in data analysis and neural network preparation through three distinct parts:</p> <ol> <li>2D Class Separability - Understanding linear vs nonlinear decision boundaries</li> <li>High-Dimensional Analysis - Exploring non-linearity in 5D space using PCA</li> <li>Real-World Data Preprocessing - Preparing Spaceship Titanic dataset for neural networks</li> </ol>"},{"location":"exercises/data/main/#1-class-separability-in-2d","title":"1) Class Separability in 2D","text":""},{"location":"exercises/data/main/#dataset-generation","title":"Dataset Generation","text":"<p>I generated a 2D dataset with four distinct classes, each following a normal distribution:</p> <ul> <li>Class 0 (Blue): Mean = [2,3], Standard Deviation = [0.8,2.5], 100 samples</li> <li>Class 1 (Orange): Mean = [5,6], Standard Deviation = [1.2,1.9], 100 samples  </li> <li>Class 2 (Green): Mean = [8,1], Standard Deviation = [0.9,0.9], 100 samples</li> <li>Class 3 (Red): Mean = [15,4], Standard Deviation = [0.5,2.0], 100 samples</li> </ul> <p>The dataset was generated using <code>numpy.random.normal()</code>. Each class has distinct characteristics designed to demonstrate different types of separability challenges.</p> <p>Dataset Generation Code</p> <pre><code>import numpy as np\nimport pandas as pd\n\n# Class 0 (Blue): Mean = [2,3], Std = [0.8,2.5]\nmu_x, mu_y = 2, 3\nstd_x, std_y = 0.8, 2.5\nn = 100\n\nX = np.random.normal(mu_x, std_x, n)\nY = np.random.normal(mu_y, std_y, n)\n\nfirst_class = pd.DataFrame({'X': X, 'Y': Y, 'Class': [0] * n})\n\n# Similar process for Classes 1, 2, and 3...\n# Class 1: Mean = [5,6], Std = [1.2,1.9]\n# Class 2: Mean = [8,1], Std = [0.9,0.9] \n# Class 3: Mean = [15,4], Std = [0.5,2.0]\n\n# Combine all classes\nsample = pd.concat([first_class, second_class, third_class, fourth_class], ignore_index=True)\n</code></pre>"},{"location":"exercises/data/main/#visualization-and-analysis","title":"Visualization and Analysis","text":"<p>Figure 1: Scatter plot showing four classes with clear color coding and proper axis labels. Each point represents a sample, with colors indicating class membership.</p> <p>Visualization Code</p> <pre><code>import matplotlib.pyplot as plt\n# Scatter plot for sample X and Y with title and legend\nfor label, group in sample.groupby('Class'):\n    plt.scatter(group['X'], group['Y'], label=f'Class {label}')\n\nplt.title('Scatter Plot of Sample Data')\nplt.xlabel('X')\nplt.ylabel('Y')\nplt.legend()\nplt.show()\n</code></pre>"},{"location":"exercises/data/main/#decision-boundaries","title":"Decision Boundaries","text":"<p>The dataset demonstrates different types of class separability that neural networks must handle:</p> <p></p> <p>Figure 2: Decision boundaries overlaid on the dataset, showing linear and nonlinear separation strategies.</p>"},{"location":"exercises/data/main/#class-separability-analysis","title":"Class Separability Analysis","text":""},{"location":"exercises/data/main/#class-3-red-linearly-separable","title":"Class 3 (Red) - Linearly Separable","text":"<ul> <li>Completely isolated with clear margin from other classes</li> <li>A simple linear boundary (vertical line at x \u2248 12) can achieve 100% separation</li> <li>Network Learning: A single perceptron could learn this boundary perfectly</li> </ul>"},{"location":"exercises/data/main/#classes-0-1-blue-orange-nonlinearly-separable","title":"Classes 0 &amp; 1 (Blue &amp; Orange) - Nonlinearly Separable","text":"<ul> <li>Significant overlap in feature space due to high variance</li> <li>Linear boundary would misclassify ~15-20% of samples</li> <li>Network Learning: Requires hidden layers to learn curved decision boundary that wraps around the overlapping region</li> </ul>"},{"location":"exercises/data/main/#class-2-green-moderately-separable","title":"Class 2 (Green) - Moderately Separable","text":"<ul> <li>Compact, low-variance cluster but positioned between other classes</li> <li>Requires elliptical/circular boundary for optimal separation</li> <li>Network Learning: Hidden layer neurons can learn radial basis-like boundaries</li> </ul>"},{"location":"exercises/data/main/#overall-complexity","title":"Overall Complexity","text":"<ul> <li>The dataset requires at least 3 distinct decision boundaries</li> <li>Linear classifier accuracy would be limited to ~75-80%</li> <li>Neural network with 2-3 hidden neurons could achieve &gt;95% accuracy</li> </ul>"},{"location":"exercises/data/main/#neural-network-implications","title":"Neural Network Implications","text":"<ul> <li>Perceptron - Can only handle linear separations (e.g., isolating Class 3)</li> <li>Multi-Layer Perceptron (MLP) - Required for nonlinear boundaries between overlapping classes</li> <li>Decision Complexity - Increases from Class 3 (linear) to Classes 0-2 (nonlinear)</li> </ul>"},{"location":"exercises/data/main/#2-non-linearity-in-higher-dimensions","title":"2) Non-Linearity in Higher Dimensions","text":""},{"location":"exercises/data/main/#5d-dataset-generation","title":"5D Dataset Generation","text":"<p>Created two classes in 5-dimensional space using multivariate normal distributions with specific covariance structures:</p>"},{"location":"exercises/data/main/#class-0-500-samples","title":"Class 0: 500 samples","text":"<ul> <li>Mean vector: <code>mu_0 = [0, 0, 0, 0, 0]</code></li> <li>Covariance matrix: Positive correlations between adjacent features   <pre><code>cov_0 = [[1.0, 0.5, 0.2, 0.1, 0.0],\n      [0.5, 1.0, 0.5, 0.2, 0.1],\n      [0.2, 0.5, 1.0, 0.5, 0.2],\n      [0.1, 0.2, 0.5, 1.0, 0.5],\n      [0.0, 0.1, 0.2, 0.5, 1.0]]\n</code></pre></li> </ul>"},{"location":"exercises/data/main/#class-1-500-samples","title":"Class 1: 500 samples","text":"<ul> <li>Mean vector: <code>mu_1 = [1.5, 1.5, 1.5, 1.5, 1.5]</code></li> <li>Covariance matrix: Mixed correlations creating complex structure   <pre><code>cov_1 = [[1.0, -0.3, 0.4, -0.2, 0.3],\n      [-0.3, 1.0, -0.2, 0.4, -0.1],\n      [0.4, -0.2, 1.0, -0.3, 0.2],\n      [-0.2, 0.4, -0.3, 1.0, -0.2],\n      [0.3, -0.1, 0.2, -0.2, 1.0]]\n</code></pre></li> </ul> <p>Both datasets were generated using <code>numpy.random.multivariate_normal()</code> with these exact parameters to ensure reproducible, realistic high-dimensional data.</p> <p>5D Dataset Generation Code</p> <pre><code>import numpy as np\nfrom sklearn.decomposition import PCA\n\n# Class 0: Mean = [0,0,0,0,0]\nmu_0 = np.zeros(5)\ncov_0 = [\n    [1.0, 0.8, 0.1, 0.0, 0.0],\n    [0.8, 1.0, 0.3, 0.0, 0.0],\n    [0.1, 0.3, 1.0, 0.5, 0.0],\n    [0.0, 0.0, 0.5, 1.0, 0.2],\n    [0.0, 0.0, 0.0, 0.2, 1.0]\n]\n\n# Class 1: Mean = [1.5,1.5,1.5,1.5,1.5]\nmu_1 = [1.5, 1.5, 1.5, 1.5, 1.5]\ncov_1 = [\n    [1.5, -0.7, 0.2, 0.0, 0.0],\n    [-0.7, 1.5, 0.4, 0.0, 0.0],\n    [0.2, 0.4, 1.5, 0.6, 0.0],\n    [0.0, 0.0, 0.6, 1.5, 0.3],\n    [0.0, 0.0, 0.0, 0.3, 1.5]\n]\n\n# Generate 500 samples for each class\nclass_0 = np.random.multivariate_normal(mu_0, cov_0, size=500)\nclass_1 = np.random.multivariate_normal(mu_1, cov_1, size=500)\n</code></pre>"},{"location":"exercises/data/main/#pca-analysis","title":"PCA Analysis","text":"<p>Applied Principal Component Analysis to project the 5D data into 2D for visualization and analysis:</p> <p></p> <p>Figure 3: 2D projection of 5D data using PCA. First two principal components capture the maximum variance while revealing class overlap.</p> <p></p> <p>Figure 4: Pairplot showing distributions along both principal components, with marginal histograms revealing the degree of class separation.</p> <p>PCA Implementation and Visualization</p> <pre><code>from sklearn.decomposition import PCA\nimport seaborn as sns\n\n# Apply PCA to reduce from 5D to 2D\nX = sample.drop('Class', axis=1)\nY = sample['Class']\n\npca = PCA(n_components=2)\npca.fit(X)\n\nprint(\"Explained variance:\")\nprint(pca.explained_variance_ratio_)\n\n# Transform data to 2D\nX_pca = pca.transform(X)\nnew_df = pd.DataFrame(X_pca, columns=['pc1', 'pc2'])\nnew_df['target'] = sample['Class']\n\n# Create pairplot with histograms\nsns.pairplot(\n    new_df, vars=['pc1', 'pc2'],\n    hue='target', diag_kind=\"hist\"\n)\nplt.show()\n</code></pre>"},{"location":"exercises/data/main/#analysis-of-the-2d-projection","title":"Analysis of the 2D Projection:","text":""},{"location":"exercises/data/main/#relationship-between-classes-a-and-b","title":"Relationship Between Classes A and B","text":"<ul> <li>Significant Overlap: The blue (Class 0) and orange (Class 1) points are heavily intermingled</li> <li>No Clear Boundary: There is no obvious linear separation between the two classes</li> <li>Complex Distribution: The classes form intertwined, curved patterns rather than distinct clusters</li> </ul>"},{"location":"exercises/data/main/#linear-separability-assessment","title":"Linear Separability Assessment","text":"<ul> <li>Not Linearly Separable: No single straight line could effectively separate the two classes</li> <li>High Misclassification: A linear classifier would incorrectly classify many points in the overlapping regions</li> <li>Curved Decision Boundary Needed: The optimal separation would require a complex, non-linear boundary</li> </ul>"},{"location":"exercises/data/main/#why-this-challenges-simple-linear-models","title":"Why This Challenges Simple Linear Models:","text":""},{"location":"exercises/data/main/#the-linear-model-problem","title":"The Linear Model Problem","text":"<p>A linear model can only create a single straight line decision boundary to separate the two classes. In this scenario with significant class overlap patterns, a linear classifier would misclassify many data points in the overlapping regions. Linear models lack the flexibility to adapt to the curved, complex boundaries needed for optimal separation.</p>"},{"location":"exercises/data/main/#why-multi-layer-neural-networks-are-needed","title":"Why Multi-Layer Neural Networks Are Needed","text":"<p>Multi-layer neural networks can handle this complex data because they have several key advantages:</p> <ul> <li>Flexible Decision Making: Unlike linear models that can only draw straight lines, neural networks can learn curved and complex boundaries that bend around the data</li> <li>Multiple Processing Layers: Each layer can transform the data in different ways, gradually building up a better understanding of how to separate the classes</li> <li>Learning from Examples: The network automatically figures out the best way to combine the original measurements to make accurate predictions</li> <li>Pattern Recognition: It can discover which combinations of the original features work best together to tell the classes apart</li> </ul> <p>This visualization demonstrates exactly why deep learning models excel at classification tasks - they can handle the complex, non-linear relationships that exist in high-dimensional data that simple linear models cannot capture.</p>"},{"location":"exercises/data/main/#3-real-world-data-preprocessing","title":"3) Real-World Data Preprocessing","text":""},{"location":"exercises/data/main/#spaceship-titanic-dataset","title":"Spaceship Titanic Dataset","text":""},{"location":"exercises/data/main/#dataset-description","title":"Dataset Description","text":"<ul> <li>Source: Kaggle Spaceship Titanic competition dataset</li> <li>Objective: Predict the <code>Transported</code> column (binary classification - whether passengers were transported to another dimension)</li> <li>Features: Mix of numerical, categorical, and text-based features</li> </ul>"},{"location":"exercises/data/main/#feature-types-identification","title":"Feature Types Identification","text":"<ul> <li>Numerical Features: <code>Age</code>, <code>RoomService</code>, <code>FoodCourt</code>, <code>ShoppingMall</code>, <code>Spa</code>, <code>VRDeck</code></li> <li>Categorical Features: <code>HomePlanet</code>, <code>CryoSleep</code>, <code>Destination</code>, <code>VIP</code></li> <li>Text/Identifier Features: <code>PassengerId</code>, <code>Name</code>, <code>Cabin</code></li> </ul> Data Exploration Code <pre><code>import pandas as pd\n\n# Load the dataset\ntrain = pd.read_csv('./data/train.csv')\ntest = pd.read_csv('./data/test.csv')\n\n# Explore data types and structure\nprint(\"Data types:\")\nprint(train.dtypes)\n\nprint(\"\\nBasic statistics:\")\nprint(train.describe())\n\nprint(\"\\nMissing values:\")\nprint(train.isnull().sum())\n</code></pre>"},{"location":"exercises/data/main/#missing-value-investigation","title":"Missing Value Investigation","text":"<p>The dataset contains missing values across several columns, requiring careful handling before neural network training. <pre><code>Missing values:\nPassengerId       0\nHomePlanet      201\nCryoSleep       217\nCabin           199\nDestination     182\nAge             179\nVIP             203\nRoomService     181\nFoodCourt       183\nShoppingMall    208\nSpa             183\nVRDeck          188\nName            200\nTransported       0\n</code></pre></p>"},{"location":"exercises/data/main/#preprocessing-implementation","title":"Preprocessing Implementation","text":"<p>Based on the provided preprocessing function, the following steps were implemented:</p>"},{"location":"exercises/data/main/#1-feature-engineering","title":"1. Feature Engineering","text":"<ul> <li>Cabin Parsing: Extracted <code>Deck</code>, <code>Num</code>, and <code>Side</code> from the cabin string (format: \"Deck/Num/Side\")</li> <li>Justification: The cabin information contains spatial patterns that could be predictive of passenger transportation</li> </ul>"},{"location":"exercises/data/main/#2-missing-value-handling-strategy","title":"2. Missing Value Handling Strategy","text":"<ul> <li>Numerical Features: Median imputation for <code>Age</code>, <code>RoomService</code>, <code>FoodCourt</code>, <code>ShoppingMall</code>, <code>Spa</code>, <code>VRDeck</code></li> <li>Categorical Features: \"Unknown\" category for missing values in <code>HomePlanet</code>, <code>Destination</code>, <code>Deck</code>, <code>Side</code></li> <li>Binary Features: Mode imputation for <code>CryoSleep</code> and <code>VIP</code></li> <li>Justification: Median imputation is robust to outliers and preserves the distribution of numerical data, while categorical imputation with \"Unknown\" maintains the categorical nature without bias.</li> </ul>"},{"location":"exercises/data/main/#3-categorical-encoding","title":"3. Categorical Encoding","text":"<ul> <li>One-Hot Encoding: Applied to <code>HomePlanet</code>, <code>Destination</code>, <code>Deck</code>, <code>Side</code></li> <li>Binary Mapping: Converted <code>CryoSleep</code> and <code>VIP</code> to {0, 1}</li> <li>Justification: Neural networks require numerical inputs; one-hot encoding preserves categorical independence without imposing ordinal relationships</li> </ul>"},{"location":"exercises/data/main/#4-feature-scaling-for-tanh-activation","title":"4. Feature Scaling for Tanh Activation","text":"<ul> <li>Standardization (Z-score): Applied to all numerical features using <code>StandardScaler</code></li> <li>Formula: <code>z = (x - \u03bc) / \u03c3</code> resulting in mean=0, standard deviation=1</li> <li>Justification: Tanh activation function operates optimally with inputs centered around zero. Standardization prevents gradient vanishing and ensures stable training.</li> </ul> <p>Complete Preprocessing Code Implementation</p> <pre><code>from sklearn.preprocessing import StandardScaler\n\ndef preprocess(df):\n    df = df.copy()\n\n    # Num\u00e9ricas\n    num_cols = [\"Age\", \"RoomService\", \"FoodCourt\", \"ShoppingMall\", \"Spa\", \"VRDeck\"]\n    for c in num_cols:\n        df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n        df[c].fillna(df[c].median(), inplace=True)\n\n    # Parsing de \"Cabin\" \n    cabin_parts = df[\"Cabin\"].astype(\"string\").str.split(\"/\", n=2, expand=True)\n    df[\"Deck\"] = cabin_parts[0]\n    df[\"Num\"]  = pd.to_numeric(cabin_parts[1], errors=\"coerce\")\n    df[\"Side\"] = cabin_parts[2]\n\n    # Muda tipo de \"Num\"\n    df[\"Num\"].fillna(df[\"Num\"].median(), inplace=True)\n\n    # Categ\u00f3ricas\n    cat_cols = [\"HomePlanet\", \"Destination\", \"Deck\", \"Side\"]\n    for c in cat_cols:\n        df[c] = df[c].astype(\"string\").fillna(\"Unknown\")\n\n    # Bin\u00e1rios ou Booleanos\n    bin_map = {True: 1, False: 0, \"True\": 1, \"False\": 0, \"true\": 1, \"false\": 0}\n    for b in [\"CryoSleep\", \"VIP\"]:\n        df[b] = df[b].map(bin_map)\n        # Se algum ainda n\u00e3o estiver preenchido, insere a Moda\n        if df[b].isna().any():\n            mode_val = df[b].mode().iloc[0] if not df[b].mode().empty else 0\n            df[b].fillna(mode_val, inplace=True)\n\n    # Drop das features que n\u00e3o ser\u00e3o utilizadas\n    drop_cols = [\"PassengerId\", \"Name\", \"Cabin\"]\n    existing_drop = [c for c in drop_cols if c in df.columns]\n    df.drop(columns=existing_drop, inplace=True)\n\n    # One-hot encode das Categ\u00f3ricas \n    X_cat = pd.get_dummies(df[cat_cols], drop_first=False, dtype=int)\n\n    # Todas as Num\u00e9ricas\n    num_all = num_cols + [\"Num\"]\n    X_num_raw = df[num_all].copy()\n\n    # Uso do Scaler pras Num\u00e9ricas\n    scaler = StandardScaler()\n    X_num = pd.DataFrame(\n        scaler.fit_transform(X_num_raw),\n        columns=num_all,\n        index=df.index\n    )\n\n    # Todas as Bin\u00e1rias\n    X_bin = df[[\"CryoSleep\", \"VIP\"]].astype(int)\n\n    # Junta tudo\n    X = pd.concat([X_num, X_bin, X_cat], axis=1)\n\n    # Retorna os dados preprocessados\n    return X\n\n# Aplica preprocessing\nX = preprocess(train)\n</code></pre>"},{"location":"exercises/data/main/#visualization-of-preprocessing-impact","title":"Visualization of Preprocessing Impact","text":"<p>Figure 5: Before and after comparison showing the effect of standardization on numerical features. The histograms demonstrate how standardization transforms the original feature distributions to have mean=0 and standard deviation=1, making them suitable for tanh activation function.</p> <p>Before/After Visualization Code</p> <pre><code># Histograms for Age and FoodCourt before scaling\nfig, axes = plt.subplots(1, 2, figsize=(12, 5))\ntrain['Age'].hist(ax=axes[0], bins=30, color='skyblue', alpha=0.7)\naxes[0].set_title('Age (Before Scaling)')\naxes[0].set_xlabel('Age')\naxes[0].set_ylabel('Frequency')\n\ntrain['FoodCourt'].hist(ax=axes[1], bins=30, color='orange', alpha=0.7)\naxes[1].set_title('FoodCourt (Before Scaling)')\naxes[1].set_xlabel('FoodCourt')\naxes[1].set_ylabel('Frequency')\nplt.tight_layout()\nplt.show()\n\n# Histograms for Age and FoodCourt after scaling\nfig, axes = plt.subplots(1, 2, figsize=(12, 5))\nX['Age'].hist(ax=axes[0], bins=30, color='skyblue', alpha=0.7)\naxes[0].set_title('Age (After Scaling)')\nX['FoodCourt'].hist(ax=axes[1], bins=30, color='orange', alpha=0.7)\naxes[1].set_title('FoodCourt (After Scaling)')\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"exercises/data/main/#why-this-preprocessing-is-essential-for-neural-networks","title":"Why This Preprocessing is Essential for Neural Networks:","text":""},{"location":"exercises/data/main/#tanh-activation-function-requirements","title":"Tanh Activation Function Requirements","text":"<ul> <li>Input Range: Tanh produces outputs in [-1, 1] and works best with standardized inputs</li> <li>Gradient Stability: Standardized features prevent gradient vanishing/exploding during backpropagation</li> <li>Training Efficiency: Features on similar scales converge faster during gradient descent</li> </ul>"},{"location":"exercises/data/main/#data-quality-for-neural-networks","title":"Data Quality for Neural Networks","text":"<ul> <li>No Missing Values: Complete dataset ensures consistent batch processing</li> <li>Numerical Consistency: All features converted to appropriate numerical format</li> <li>Categorical Handling: One-hot encoding creates binary features that neural networks can interpret effectively</li> </ul>"},{"location":"exercises/perceptron/main/","title":"Exercise 2 - Perceptron Implementation","text":""},{"location":"exercises/perceptron/main/#overview","title":"Overview","text":"<p>This exercise explores the fundamental concepts of perceptron learning and its limitations through two distinct scenarios:</p> <ol> <li>Well-Separated Classes - Understanding perceptron convergence with linearly separable data</li> <li>Overlapping Classes - Exploring perceptron limitations with non-separable data</li> </ol> <p>The exercise demonstrates the perceptron convergence theorem in practice and highlights why more complex neural networks are needed for real-world classification problems.</p>"},{"location":"exercises/perceptron/main/#exercise-1-linearly-separable-data","title":"Exercise 1: Linearly Separable Data","text":""},{"location":"exercises/perceptron/main/#dataset-generation","title":"Dataset Generation","text":"<p>I generated a 2D dataset with two well-separated classes, each following a normal distribution:</p> <ul> <li>Class 0 (Blue): Mean = [1.5, 1.5], Covariance Matrix = [[0.5, 0], [0, 0.5]], 1000 samples</li> <li>Class 1 (Orange): Mean = [5, 5], Covariance Matrix = [[0.5, 0], [0, 0.5]], 1000 samples</li> </ul> <p>The dataset was generated using <code>numpy.random.multivariate_normal()</code> with these exact parameters to ensure reproducible results and clear class separation.</p> <p></p> <p>Figure 1: Exercise 1 dataset showing two well-separated classes with clear linear separability. Class 0 (blue) centered at [1.5, 1.5] and Class 1 (orange) centered at [5, 5].</p> <p>Dataset Generation Code</p> <pre><code>import numpy as np\nimport pandas as pd\n\n# Class 0: Mean = [1.5, 1.5], Covariance = [[0.5, 0], [0, 0.5]]\nmu = [1.5, 1.5]\ncov = [[0.5, 0], [0, 0.5]]  # Diagonal covariance\n\nclass_0 = np.random.multivariate_normal(mu, cov, 1000)\nsample['feature1'] = class_0[:, 0]\nsample['feature2'] = class_0[:, 1]\nsample['label'] = 0\n\n# Class 1: Mean = [5, 5], Covariance = [[0.5, 0], [0, 0.5]]\nmu = [5, 5]\ncov = [[0.5, 0], [0, 0.5]]  # Diagonal covariance\n\nclass_1 = np.random.multivariate_normal(mu, cov, 1000)\ntemp = pd.DataFrame()\ntemp['feature1'] = class_1[:, 0]\ntemp['feature2'] = class_1[:, 1]\ntemp['label'] = 1\nsample = pd.concat([sample, temp], ignore_index=True)\n</code></pre>"},{"location":"exercises/perceptron/main/#perceptron-implementation-from-scratch","title":"Perceptron Implementation from Scratch","text":"<p>Following the exercise requirements, I implemented a single-layer perceptron using only NumPy for basic linear algebra operations:</p> <p>Perceptron Implementation</p> <pre><code>class Perceptron:\n    def __init__(self, learning_rate=0.01, n_iters=100):\n        self.learning_rate = learning_rate\n        self.n_iters = n_iters\n        self.weights = None\n        self.bias = None\n        self.accuracies = []\n\n    def fit(self, X, y):\n        # Initialize weights and bias\n        n_features = X.shape[1]\n        self.weights = np.zeros(n_features)\n        self.bias = 0\n\n        # Convert labels to -1 and 1 for perceptron algorithm\n        y_train = np.where(y == 0, -1, 1)\n\n        # Training loop\n        for epoch in range(self.n_iters):\n            errors = 0\n\n            for i in range(X.shape[0]):\n                # Calculate prediction\n                linear_output = np.dot(X[i], self.weights) + self.bias\n                y_pred = 1 if linear_output &gt;= 0 else -1\n\n                # Update weights if misclassified\n                if y_train[i] != y_pred:\n                    self.weights += self.learning_rate * y_train[i] * X[i]\n                    self.bias += self.learning_rate * y_train[i]\n                    errors += 1\n\n            # Calculate accuracy for this epoch\n            accuracy = self.score(X, y)\n            self.accuracies.append(accuracy)\n\n            # Check for convergence\n            if errors == 0:\n                print(f\"Converged at epoch {epoch + 1}\")\n                break\n\n    def predict(self, X):\n        linear_output = np.dot(X, self.weights) + self.bias\n        return np.where(linear_output &gt;= 0, 1, 0)\n\n    def score(self, X, y):\n        predictions = self.predict(X)\n        return np.mean(predictions == y)\n</code></pre>"},{"location":"exercises/perceptron/main/#training-results","title":"Training Results","text":""},{"location":"exercises/perceptron/main/#the-perceptron-was-trained-with-the-following-parameters","title":"The perceptron was trained with the following parameters:","text":"<ul> <li>Learning Rate: \u03b7 = 0.01</li> <li>Maximum Epochs: 100</li> <li>Convergence Criterion: No weight updates in a full pass over the dataset</li> </ul>"},{"location":"exercises/perceptron/main/#training-performance","title":"Training Performance","text":"<ul> <li>Convergence: Achieved at epoch 12</li> <li>Final Accuracy: 100.00% (2000/2000 samples correctly classified)</li> <li>Final Weights: [0.0199, 0.0171]</li> <li>Final Bias: -0.1200</li> <li>Misclassified Points: 0</li> </ul>"},{"location":"exercises/perceptron/main/#visualization-and-analysis","title":"Visualization and Analysis","text":"<p>The training progress and decision boundary visualization demonstrate the perceptron's effectiveness on linearly separable data:</p> <p></p> <p>Figure 2: Training accuracy progression for Exercise 1, showing rapid convergence to 100% accuracy in just 12 epochs.</p> <p>Training Accuracy Plot Code</p> <pre><code># Plot accuracies\nplt.plot(range(1, len(perceptron.accuracies) + 1), perceptron.accuracies, marker='o')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.title('Perceptron Training Accuracy over Epochs')\nplt.show()\n</code></pre> <p>Training Accuracy Progress</p> <p>The perceptron achieved rapid convergence, reaching 100% accuracy in just 12 epochs. The accuracy curve shows a steady improvement from the initial random state to perfect classification.</p> <p></p> <p>Figure 3: Decision boundary visualization for Exercise 1. The linear boundary perfectly separates the two classes with a clear margin, demonstrating successful linearly separable classification.</p> <p>Decision Boundary Plot Code</p> <pre><code># Decision boundary visualization\nh = 0.02  # step size in the mesh\n\nx_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\ny_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                     np.arange(y_min, y_max, h))\nZ = perceptron.predict(np.c_[xx.ravel(), yy.ravel()])\nZ = Z.reshape(xx.shape)\n\nplt.contourf(xx, yy, Z, alpha=0.2, cmap='Set1')\nplt.title('Perceptron Decision Boundary')\nplt.xlabel('Feature 1')\nplt.ylabel('Feature 2')\nsns.scatterplot(data=sample, x='feature1', y='feature2', hue='label', palette='Set1', edgecolor='k')\nplt.show()\n</code></pre> <p>Decision Boundary Analysis</p> <p>The learned decision boundary (defined by w\u00b7x + b = 0) perfectly separates the two classes with a clear margin. The boundary equation is: <pre><code>0.0199 * feature1 + 0.0171 * feature2 - 0.1200 = 0\n</code></pre></p> <p>This can be simplified to: <code>1.163 * feature1 + feature2 = 7.018</code>, which creates an effective separation between the class means at [1.5, 1.5] and [5, 5].</p>"},{"location":"exercises/perceptron/main/#why-linearly-separable-data-leads-to-quick-convergence","title":"Why Linearly Separable Data Leads to Quick Convergence","text":"<p>The rapid convergence observed in Exercise 1 is explained by the Perceptron Convergence Theorem:</p>"},{"location":"exercises/perceptron/main/#theoretical-foundation","title":"Theoretical Foundation","text":"<ul> <li>Linearly Separable Data: The two classes can be perfectly separated by a linear decision boundary</li> <li>Finite Convergence: The perceptron algorithm is guaranteed to find a separating hyperplane in finite steps</li> <li>Error-Driven Learning: Each misclassification moves the decision boundary closer to the optimal solution</li> </ul>"},{"location":"exercises/perceptron/main/#practical-factors-contributing-to-fast-convergence","title":"Practical Factors Contributing to Fast Convergence","text":"<ol> <li>Large Margin: The 3.5-unit distance between class means creates a wide separable margin</li> <li>Low Variance: Covariance of 0.5 keeps most samples close to their class centers</li> <li>Minimal Overlap: The combination of distant means and low variance ensures clean separation</li> <li>Optimal Learning Rate: \u03b7 = 0.01 provides stable updates without overshooting</li> </ol>"},{"location":"exercises/perceptron/main/#exercise-2-overlapping-classes","title":"Exercise 2: Overlapping Classes","text":""},{"location":"exercises/perceptron/main/#dataset-generation_1","title":"Dataset Generation","text":"<p>For Exercise 2, I generated a more challenging dataset with overlapping classes:</p> <ul> <li>Class 0 (Blue): Mean = [3, 3], Covariance Matrix = [[1.5, 0], [0, 1.5]], 1000 samples</li> <li>Class 1 (Orange): Mean = [4, 4], Covariance Matrix = [[1.5, 0], [0, 1.5]], 1000 samples</li> </ul> <p>This configuration creates significant class overlap due to: - Closer Means: Only 1.41 units apart (\u221a2 distance) compared to 4.95 units in Exercise 1 - Higher Variance: 1.5 variance creates wider distributions with more spread</p> <p></p> <p>Figure 4: Exercise 2 dataset showing overlapping classes with significant intersection between Class 0 (blue) and Class 1 (orange), making linear separation impossible.</p> <p>Dataset Visualization Code</p> <pre><code># Plot the overlapping dataset for Exercise 2\nplt.figure(figsize=(8, 6))\nsns.scatterplot(data=sample[0], x='feature1', y='feature2', hue='label', palette='Set1')\nplt.title('Exercise 2: Overlapping Classes Dataset')\nplt.xlabel('Feature 1')\nplt.ylabel('Feature 2')\nplt.show()\n</code></pre> <p>Dataset Characteristics</p> <pre><code># Class 0: Mean = [3, 3], Covariance = [[1.5, 0], [0, 1.5]]\nmu = [3, 3]\ncov = [[1.5, 0], [0, 1.5]]\nclass_0_run = np.random.multivariate_normal(mu, cov, 1000)\n\n# Class 1: Mean = [4, 4], Covariance = [[1.5, 0], [0, 1.5]]\nmu = [4, 4]\ncov = [[1.5, 0], [0, 1.5]]\nclass_1_run = np.random.multivariate_normal(mu, cov, 1000)\n</code></pre>"},{"location":"exercises/perceptron/main/#training-results-on-overlapping-data","title":"Training Results on Overlapping Data","text":"<p>Using the same perceptron implementation and training parameters as Exercise 1:</p>"},{"location":"exercises/perceptron/main/#training-performance_1","title":"Training Performance","text":"<ul> <li>Convergence: Failed to converge within 100 epochs</li> <li>Final Accuracy: 50.25% (1005/2000 samples correctly classified)</li> <li>Final Weights: [0.42, 0.42]</li> <li>Final Bias: -0.58</li> <li>Misclassified Points: 995 (49.75% error rate)</li> </ul>"},{"location":"exercises/perceptron/main/#multiple-runs-analysis","title":"Multiple Runs Analysis","text":"<p>To assess consistency, I ran the experiment 5 times with different random seeds:</p> Run Accuracy Epochs Converged 1 50.90% 100 No 2 51.10% 100 No 3 50.85% 100 No 4 50.05% 100 No 5 50.25% 100 No <p>Summary: Average accuracy = 50.63% \u00b1 0.41%, showing consistent failure across all runs.</p>"},{"location":"exercises/perceptron/main/#visualization-of-overlapping-classes","title":"Visualization of Overlapping Classes","text":"<p>Figure 5: Training accuracy for Exercise 2 showing oscillating behavior and failure to converge, with accuracy hovering around 50% throughout 100 epochs.</p> <p>Training Accuracy Plot Code</p> <pre><code># Plot accuracies\nplt.plot(range(1, len(perceptron.accuracies) + 1), perceptron.accuracies, marker='o')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.title('Perceptron Training Accuracy over Epochs')\nplt.show()\n</code></pre> <p></p> <p>Decision Boundary Visualization Code</p> <pre><code># Highlight misclassified points\npredictions = perceptron.predict(X)\nmisclassified_mask = predictions != y\ncorrectly_classified_mask = predictions == y\n\nplt.figure(figsize=(10, 8))\n# Plot correctly classified points\nscatter1 = plt.scatter(X[correctly_classified_mask, 0], X[correctly_classified_mask, 1], \n           c=y[correctly_classified_mask], cmap='Set1', alpha=0.7, s=50, \n           label='Correctly Classified', edgecolors='black', linewidth=0.5)\n\n# Plot misclassified points with different markers\nplt.scatter(X[misclassified_mask, 0], X[misclassified_mask, 1], \n           c=y[misclassified_mask], cmap='Set1', alpha=0.9, s=100, \n           marker='x', linewidth=3, label='Misclassified')\n\n# Plot decision boundary\nh = 0.02\nx_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\ny_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                     np.arange(y_min, y_max, h))\nZ = perceptron.predict(np.c_[xx.ravel(), yy.ravel()])\nZ = Z.reshape(xx.shape)\nplt.contour(xx, yy, Z, levels=[0.5], colors='red', linestyles='--', linewidths=2)\n\nplt.title('Exercise 2: Decision Boundary with Misclassified Points Highlighted')\nplt.xlabel('Feature 1')\nplt.ylabel('Feature 2')\nplt.legend()\nplt.show()\n</code></pre> <p>Figure 6: Decision boundary for Exercise 2 with misclassified points highlighted. The linear boundary cannot properly separate the overlapping classes, resulting in numerous misclassifications (shown as X markers).</p> <p>The decision boundary visualization reveals the fundamental limitation of linear classifiers on non-separable data:</p> <p>Decision Boundary Limitations</p> <p>The perceptron's linear decision boundary cuts through the overlapping region, inevitably misclassifying samples from both classes. The boundary oscillates during training as it encounters contradictory examples in the overlap zone.</p>"},{"location":"exercises/perceptron/main/#comparative-analysis-exercise-1-vs-exercise-2","title":"Comparative Analysis: Exercise 1 vs Exercise 2","text":"Aspect Exercise 1 (Separable) Exercise 2 (Overlapping) Class Distance 4.95 units 1.41 units Variance 0.5 (low spread) 1.5 (high spread) Convergence 12 epochs Never (100+ epochs) Final Accuracy 100.00% 50.63% \u00b1 0.41% Decision Boundary Stable, optimal Oscillating, suboptimal Misclassifications 0 points ~988 points <p>Figure 7: Side-by-side comparison of the two datasets, clearly showing the difference between linearly separable (left) and overlapping (right) classes.</p> <p></p> <p>Figure 8: Training accuracy comparison between Exercise 1 (blue) and Exercise 2 (orange), demonstrating the stark difference in convergence behavior between separable and non-separable data.</p>"},{"location":"exercises/perceptron/main/#why-exercise-2-failed-to-converge","title":"Why Exercise 2 Failed to Converge","text":""},{"location":"exercises/perceptron/main/#mathematical-impossibility","title":"Mathematical Impossibility","text":"<p>The failure to converge in Exercise 2 stems from a fundamental mathematical limitation:</p> <ol> <li>No Linear Separator Exists: The overlapping distributions cannot be perfectly separated by any straight line</li> <li>Contradictory Updates: The perceptron encounters points from different classes in the same region, leading to conflicting weight updates</li> <li>Perpetual Oscillation: The algorithm continuously adjusts weights in response to misclassifications, never reaching a stable state</li> </ol>"},{"location":"exercises/perceptron/main/#the-perceptron-limitation","title":"The Perceptron Limitation","text":"<p>This exercise perfectly demonstrates the Perceptron Limitation Theorem: perceptrons can only solve linearly separable problems. When data is not linearly separable: - The algorithm will never converge - Accuracy will remain poor (often near random guessing) - The decision boundary will oscillate indefinitely</p>"},{"location":"exercises/perceptron/main/#implications-for-neural-network-design","title":"Implications for Neural Network Design","text":""},{"location":"exercises/perceptron/main/#why-multi-layer-networks-are-essential","title":"Why Multi-Layer Networks Are Essential","text":"<p>The results from Exercise 2 highlight why modern neural networks use multiple layers:</p> <ol> <li>Non-Linear Transformations: Hidden layers can transform the input space to make classes separable</li> <li>Feature Learning: Multiple layers can learn complex feature combinations that reveal hidden patterns</li> <li>Universal Approximation: Deep networks can approximate any continuous function, handling arbitrary decision boundaries</li> </ol>"},{"location":"projects/classification/main/","title":"Project 1 - Classification","text":"<p>Aqui vai toda a documenta\u00e7\u00e3o do projeto, incluindo o que j\u00e1 foi feito e o que falta fazer.</p>"},{"location":"thisdocumentation/main/","title":"Documentation","text":""},{"location":"thisdocumentation/main/#pre-requisitos","title":"Pr\u00e9-requisitos","text":"<p>Antes de come\u00e7ar, certifique-se de que voc\u00ea possui os seguintes pr\u00e9-requisitos instalados em seu sistema:</p> <ul> <li>Git: Para clonar o reposit\u00f3rio.</li> </ul>"},{"location":"thisdocumentation/main/#instalando-o-python","title":"Instalando o Python","text":"LinuxmacOSWindows <p>Instale o Python 3.8 ou superior.</p> <pre><code>sudo apt install python3 python3-venv python3-pip\npython3 --version\n</code></pre> <p>Instale o Python 3.8 ou superior.</p> <pre><code>brew install python\npython3 --version\n</code></pre> <p>Instale o Python 3.13 ou superior. Baixe o instalador do site oficial do Python (https://www.python.org/downloads/) e execute-o. Certifique-se de marcar a op\u00e7\u00e3o \"Add Python to PATH\" durante a instala\u00e7\u00e3o.</p> <pre><code>python --version\n</code></pre>"},{"location":"thisdocumentation/main/#usage","title":"Usage","text":"<p>Para utilizar o c\u00f3digo deste reposit\u00f3rio, siga as instru\u00e7\u00f5es a seguir:</p> <p>Clone ou fork este reposit\u00f3rio:</p> <pre><code>git clone &lt;URL_DO_REPOSITORIO&gt;\n</code></pre> <p>Crie um ambiente virtual do Python:</p> Linux/macOSWindows <pre><code>python3 -m venv env\n</code></pre> <pre><code>python -m venv env\n</code></pre> <p>Ative o ambiente virtual (voc\u00ea deve fazer isso sempre que for executar algum script deste reposit\u00f3rio):</p> Linux/macOSWindows <pre><code>source ./env/bin/activate\n</code></pre> <pre><code>.\\env\\Scripts\\activate\n</code></pre> <p>Instale as depend\u00eancias com:</p> Linux/macOSWindows <pre><code>python3 -m pip install -r requirements.txt --upgrade\n</code></pre> <pre><code>python -m pip install -r requirements.txt --upgrade\n</code></pre>"},{"location":"thisdocumentation/main/#deployment","title":"Deployment","text":"<p>O material utiliza o mkdocs para gerar a documenta\u00e7\u00e3o. Para visualizar a documenta\u00e7\u00e3o, execute o comando:</p> <pre><code>mkdocs serve -o\n</code></pre> <p>Para subir ao GitHub Pages, execute o comando:</p> <pre><code>mkdocs gh-deploy\n</code></pre> <p>Esse reposit\u00f3rio possui um workflow do GitHub Actions que executa o comando <code>mkdocs gh-deploy</code> sempre que houver um push na branch <code>main</code>. Assim, n\u00e3o \u00e9 necess\u00e1rio executar esse comando manualmente. Toda vez que voc\u00ea fizer um push na branch <code>main</code>, a documenta\u00e7\u00e3o ser\u00e1 atualizada automaticamente no GitHub Pages.</p> <p>Aviso 1</p> <p>Para que o github actions funcione corretamente, \u00e9 necess\u00e1rio que o reposit\u00f3rio esteja configurado para que o bot <code>github-actions[bot]</code> tenha permiss\u00e3o de escrita. Voc\u00ea pode verificar isso nas configura\u00e7\u00f5es do reposit\u00f3rio, na se\u00e7\u00e3o \"Actions\" e depois em \"General\". Certifique-se de que a op\u00e7\u00e3o \"Workflow permissions\" esteja definida como \"Read and write permissions\".</p> <p></p> <p>Aviso 2</p> <p>Depois de publicar, caso n\u00e3o consiga acessar a p\u00e1gina, verifique se o github pages est\u00e1 configurado corretamente. V\u00e1 at\u00e9 as configura\u00e7\u00f5es do reposit\u00f3rio, na se\u00e7\u00e3o \"Pages\" e verifique se a branch <code>gh-pages</code> est\u00e1 selecionada como fonte. Se n\u00e3o estiver, selecione-a e salve as altera\u00e7\u00f5es.</p> <p></p> <p>Pay Attention</p> <p>No arquivo '<code>mkdocs.yml</code>, a se\u00e7\u00e3o <code>site_url</code> deve estar configurada corretamente para o seu reposit\u00f3rio. Por exemplo, se o seu reposit\u00f3rio estiver em <code>https://github.com/usuario/repositorio</code>, a se\u00e7\u00e3o <code>site_url</code> deve ser:</p> <pre><code>site_url: https://usuario.github.io/repositorio\n</code></pre> <p>Tamb\u00e9m, certifique-se de que a se\u00e7\u00e3o <code>repo_url</code> esteja configurada corretamente para o seu reposit\u00f3rio. Por exemplo:</p> <pre><code>repo_url: https://github.com/usuario/repositorio\n</code></pre>"}]}