{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Neural Networks &amp; Deep Learning Portfolio","text":"Academic Term <p>2025.1</p>"},{"location":"#author","title":"Author","text":"<p>Leonardo Teixeira Artificial Neural Networks and Deep Learning Course</p>"},{"location":"#portfolio-overview","title":"Portfolio Overview","text":"<p>This portfolio documents my journey through the Artificial Neural Networks and Deep Learning course, showcasing both theoretical understanding and practical implementation of neural network concepts.</p> <p>Portfolio Structure</p> <p>This portfolio is organized into Exercises and Projects. Each section demonstrates different aspects of neural network design, implementation, and analysis. Navigate through the sidebar to explore specific topics.</p>"},{"location":"#progress-tracker","title":"Progress Tracker","text":""},{"location":"#exercises","title":"Exercises","text":"<ul> <li> Exercise 1 - Data \u2705 Completed</li> <li> Exercise 2 - Perceptron \u2705 Completed</li> <li> Exercise 3 - MLP</li> <li> Exercise 4 - Metrics</li> </ul>"},{"location":"#projects","title":"Projects","text":"<ul> <li> Project 1 - Classification</li> </ul>"},{"location":"#learning-objectives","title":"Learning Objectives","text":"<p>Throughout this course, I aim to develop expertise in:</p>"},{"location":"#1-data-analysis-preprocessing","title":"1. Data Analysis &amp; Preprocessing","text":"<ul> <li>Understanding class separability</li> <li>Feature engineering for neural networks</li> <li>Handling real-world datasets</li> </ul>"},{"location":"#2-neural-network-fundamentals","title":"2. Neural Network Fundamentals","text":"<ul> <li>Perceptrons vs Multi-Layer Perceptrons</li> <li>Activation functions and their properties</li> <li>Forward and backward propagation</li> </ul>"},{"location":"#3-training-optimization","title":"3. Training &amp; Optimization","text":"<ul> <li>Loss functions and optimization algorithms</li> <li>Regularization techniques</li> <li>Hyperparameter tuning</li> </ul>"},{"location":"#4-advanced-architectures","title":"4. Advanced Architectures","text":"<ul> <li>Convolutional Neural Networks (CNNs)</li> <li>Recurrent Neural Networks (RNNs)</li> <li>Modern architectures and techniques</li> </ul>"},{"location":"#technical-skills-demonstrated","title":"Technical Skills Demonstrated","text":""},{"location":"#programming-tools","title":"Programming &amp; Tools","text":"<ul> <li>Python: NumPy, Pandas, Matplotlib, Seaborn</li> <li>Machine Learning: Scikit-learn, TensorFlow/PyTorch</li> <li>Data Visualization: Statistical plots, PCA analysis</li> <li>Documentation: Markdown, MkDocs, Jupyter Notebooks</li> </ul>"},{"location":"#analytical-skills","title":"Analytical Skills","text":"<ul> <li>Statistical data analysis</li> <li>Dimensionality reduction techniques</li> <li>Class separability assessment</li> <li>Feature engineering strategies</li> </ul>"},{"location":"#navigation-guide","title":"Navigation Guide","text":"<p>Use the sidebar to explore:</p> <ul> <li>Exercises: Detailed implementations and analyses</li> <li>Projects: Comprehensive applications of learned concepts  </li> <li>Documentation: Technical details about this portfolio</li> </ul> <p>Each exercise includes: - Problem statements and objectives - Step-by-step implementations - Visualizations and results - Key insights and learnings - Code examples with explanations</p>"},{"location":"#contact-collaboration","title":"Contact &amp; Collaboration","text":"<p>This portfolio represents my academic work in neural networks and deep learning. Feel free to explore the implementations and analyses documented here.</p> <p>GitHub Repository: ann-dl-portfolio</p>"},{"location":"#references","title":"References","text":"<p>Material for MkDocs</p>"},{"location":"exercises/data/main/","title":"Exercise 1 - Data Analysis","text":""},{"location":"exercises/data/main/#overview","title":"Overview","text":"<p>This exercise explores fundamental concepts in data analysis and neural network preparation through three distinct parts:</p> <ol> <li>2D Class Separability - Understanding linear vs nonlinear decision boundaries</li> <li>High-Dimensional Analysis - Exploring non-linearity in 5D space using PCA</li> <li>Real-World Data Preprocessing - Preparing Spaceship Titanic dataset for neural networks</li> </ol>"},{"location":"exercises/data/main/#1-class-separability-in-2d","title":"1) Class Separability in 2D","text":""},{"location":"exercises/data/main/#dataset-generation","title":"Dataset Generation","text":"<p>I generated a 2D dataset with four distinct classes, each following a normal distribution:</p> <ul> <li>Class 0 (Blue): Mean = [2,3], Standard Deviation = [0.8,2.5], 100 samples</li> <li>Class 1 (Orange): Mean = [5,6], Standard Deviation = [1.2,1.9], 100 samples  </li> <li>Class 2 (Green): Mean = [8,1], Standard Deviation = [0.9,0.9], 100 samples</li> <li>Class 3 (Red): Mean = [15,4], Standard Deviation = [0.5,2.0], 100 samples</li> </ul> <p>The dataset was generated using <code>numpy.random.normal()</code>. Each class has distinct characteristics designed to demonstrate different types of separability challenges.</p> <p>Dataset Generation Code</p> <pre><code>import numpy as np\nimport pandas as pd\n\n# Class 0 (Blue): Mean = [2,3], Std = [0.8,2.5]\nmu_x, mu_y = 2, 3\nstd_x, std_y = 0.8, 2.5\nn = 100\n\nX = np.random.normal(mu_x, std_x, n)\nY = np.random.normal(mu_y, std_y, n)\n\nfirst_class = pd.DataFrame({'X': X, 'Y': Y, 'Class': [0] * n})\n\n# Similar process for Classes 1, 2, and 3...\n# Class 1: Mean = [5,6], Std = [1.2,1.9]\n# Class 2: Mean = [8,1], Std = [0.9,0.9] \n# Class 3: Mean = [15,4], Std = [0.5,2.0]\n\n# Combine all classes\nsample = pd.concat([first_class, second_class, third_class, fourth_class], ignore_index=True)\n</code></pre>"},{"location":"exercises/data/main/#visualization-and-analysis","title":"Visualization and Analysis","text":"<p>Figure 1: Scatter plot showing four classes with clear color coding and proper axis labels. Each point represents a sample, with colors indicating class membership.</p> <p>Visualization Code</p> <pre><code>import matplotlib.pyplot as plt\n# Scatter plot for sample X and Y with title and legend\nfor label, group in sample.groupby('Class'):\n    plt.scatter(group['X'], group['Y'], label=f'Class {label}')\n\nplt.title('Scatter Plot of Sample Data')\nplt.xlabel('X')\nplt.ylabel('Y')\nplt.legend()\nplt.show()\n</code></pre>"},{"location":"exercises/data/main/#decision-boundaries","title":"Decision Boundaries","text":"<p>The dataset demonstrates different types of class separability that neural networks must handle:</p> <p></p> <p>Figure 2: Decision boundaries overlaid on the dataset, showing linear and nonlinear separation strategies.</p>"},{"location":"exercises/data/main/#class-separability-analysis","title":"Class Separability Analysis","text":""},{"location":"exercises/data/main/#class-3-red-linearly-separable","title":"Class 3 (Red) - Linearly Separable","text":"<ul> <li>Completely isolated with clear margin from other classes</li> <li>A simple linear boundary (vertical line at x \u2248 12) can achieve 100% separation</li> <li>Network Learning: A single perceptron could learn this boundary perfectly</li> </ul>"},{"location":"exercises/data/main/#classes-0-1-blue-orange-nonlinearly-separable","title":"Classes 0 &amp; 1 (Blue &amp; Orange) - Nonlinearly Separable","text":"<ul> <li>Significant overlap in feature space due to high variance</li> <li>Linear boundary would misclassify ~15-20% of samples</li> <li>Network Learning: Requires hidden layers to learn curved decision boundary that wraps around the overlapping region</li> </ul>"},{"location":"exercises/data/main/#class-2-green-moderately-separable","title":"Class 2 (Green) - Moderately Separable","text":"<ul> <li>Compact, low-variance cluster but positioned between other classes</li> <li>Requires elliptical/circular boundary for optimal separation</li> <li>Network Learning: Hidden layer neurons can learn radial basis-like boundaries</li> </ul>"},{"location":"exercises/data/main/#overall-complexity","title":"Overall Complexity","text":"<ul> <li>The dataset requires at least 3 distinct decision boundaries</li> <li>Linear classifier accuracy would be limited to ~75-80%</li> <li>Neural network with 2-3 hidden neurons could achieve &gt;95% accuracy</li> </ul>"},{"location":"exercises/data/main/#neural-network-implications","title":"Neural Network Implications","text":"<ul> <li>Perceptron - Can only handle linear separations (e.g., isolating Class 3)</li> <li>Multi-Layer Perceptron (MLP) - Required for nonlinear boundaries between overlapping classes</li> <li>Decision Complexity - Increases from Class 3 (linear) to Classes 0-2 (nonlinear)</li> </ul>"},{"location":"exercises/data/main/#2-non-linearity-in-higher-dimensions","title":"2) Non-Linearity in Higher Dimensions","text":""},{"location":"exercises/data/main/#5d-dataset-generation","title":"5D Dataset Generation","text":"<p>Created two classes in 5-dimensional space using multivariate normal distributions with specific covariance structures:</p>"},{"location":"exercises/data/main/#class-0-500-samples","title":"Class 0: 500 samples","text":"<ul> <li>Mean vector: <code>mu_0 = [0, 0, 0, 0, 0]</code></li> <li>Covariance matrix: Positive correlations between adjacent features   <pre><code>cov_0 = [[1.0, 0.5, 0.2, 0.1, 0.0],\n      [0.5, 1.0, 0.5, 0.2, 0.1],\n      [0.2, 0.5, 1.0, 0.5, 0.2],\n      [0.1, 0.2, 0.5, 1.0, 0.5],\n      [0.0, 0.1, 0.2, 0.5, 1.0]]\n</code></pre></li> </ul>"},{"location":"exercises/data/main/#class-1-500-samples","title":"Class 1: 500 samples","text":"<ul> <li>Mean vector: <code>mu_1 = [1.5, 1.5, 1.5, 1.5, 1.5]</code></li> <li>Covariance matrix: Mixed correlations creating complex structure   <pre><code>cov_1 = [[1.0, -0.3, 0.4, -0.2, 0.3],\n      [-0.3, 1.0, -0.2, 0.4, -0.1],\n      [0.4, -0.2, 1.0, -0.3, 0.2],\n      [-0.2, 0.4, -0.3, 1.0, -0.2],\n      [0.3, -0.1, 0.2, -0.2, 1.0]]\n</code></pre></li> </ul> <p>Both datasets were generated using <code>numpy.random.multivariate_normal()</code> with these exact parameters to ensure reproducible, realistic high-dimensional data.</p> <p>5D Dataset Generation Code</p> <pre><code>import numpy as np\nfrom sklearn.decomposition import PCA\n\n# Class 0: Mean = [0,0,0,0,0]\nmu_0 = np.zeros(5)\ncov_0 = [\n    [1.0, 0.8, 0.1, 0.0, 0.0],\n    [0.8, 1.0, 0.3, 0.0, 0.0],\n    [0.1, 0.3, 1.0, 0.5, 0.0],\n    [0.0, 0.0, 0.5, 1.0, 0.2],\n    [0.0, 0.0, 0.0, 0.2, 1.0]\n]\n\n# Class 1: Mean = [1.5,1.5,1.5,1.5,1.5]\nmu_1 = [1.5, 1.5, 1.5, 1.5, 1.5]\ncov_1 = [\n    [1.5, -0.7, 0.2, 0.0, 0.0],\n    [-0.7, 1.5, 0.4, 0.0, 0.0],\n    [0.2, 0.4, 1.5, 0.6, 0.0],\n    [0.0, 0.0, 0.6, 1.5, 0.3],\n    [0.0, 0.0, 0.0, 0.3, 1.5]\n]\n\n# Generate 500 samples for each class\nclass_0 = np.random.multivariate_normal(mu_0, cov_0, size=500)\nclass_1 = np.random.multivariate_normal(mu_1, cov_1, size=500)\n</code></pre>"},{"location":"exercises/data/main/#pca-analysis","title":"PCA Analysis","text":"<p>Applied Principal Component Analysis to project the 5D data into 2D for visualization and analysis:</p> <p></p> <p>Figure 3: 2D projection of 5D data using PCA. First two principal components capture the maximum variance while revealing class overlap.</p> <p></p> <p>Figure 4: Pairplot showing distributions along both principal components, with marginal histograms revealing the degree of class separation.</p> <p>PCA Implementation and Visualization</p> <pre><code>from sklearn.decomposition import PCA\nimport seaborn as sns\n\n# Apply PCA to reduce from 5D to 2D\nX = sample.drop('Class', axis=1)\nY = sample['Class']\n\npca = PCA(n_components=2)\npca.fit(X)\n\nprint(\"Explained variance:\")\nprint(pca.explained_variance_ratio_)\n\n# Transform data to 2D\nX_pca = pca.transform(X)\nnew_df = pd.DataFrame(X_pca, columns=['pc1', 'pc2'])\nnew_df['target'] = sample['Class']\n\n# Create pairplot with histograms\nsns.pairplot(\n    new_df, vars=['pc1', 'pc2'],\n    hue='target', diag_kind=\"hist\"\n)\nplt.show()\n</code></pre>"},{"location":"exercises/data/main/#analysis-of-the-2d-projection","title":"Analysis of the 2D Projection:","text":""},{"location":"exercises/data/main/#relationship-between-classes-a-and-b","title":"Relationship Between Classes A and B","text":"<ul> <li>Significant Overlap: The blue (Class 0) and orange (Class 1) points are heavily intermingled</li> <li>No Clear Boundary: There is no obvious linear separation between the two classes</li> <li>Complex Distribution: The classes form intertwined, curved patterns rather than distinct clusters</li> </ul>"},{"location":"exercises/data/main/#linear-separability-assessment","title":"Linear Separability Assessment","text":"<ul> <li>Not Linearly Separable: No single straight line could effectively separate the two classes</li> <li>High Misclassification: A linear classifier would incorrectly classify many points in the overlapping regions</li> <li>Curved Decision Boundary Needed: The optimal separation would require a complex, non-linear boundary</li> </ul>"},{"location":"exercises/data/main/#why-this-challenges-simple-linear-models","title":"Why This Challenges Simple Linear Models:","text":""},{"location":"exercises/data/main/#the-linear-model-problem","title":"The Linear Model Problem","text":"<p>A linear model can only create a single straight line decision boundary to separate the two classes. In this scenario with significant class overlap patterns, a linear classifier would misclassify many data points in the overlapping regions. Linear models lack the flexibility to adapt to the curved, complex boundaries needed for optimal separation.</p>"},{"location":"exercises/data/main/#why-multi-layer-neural-networks-are-needed","title":"Why Multi-Layer Neural Networks Are Needed","text":"<p>Multi-layer neural networks can handle this complex data because they have several key advantages:</p> <ul> <li>Flexible Decision Making: Unlike linear models that can only draw straight lines, neural networks can learn curved and complex boundaries that bend around the data</li> <li>Multiple Processing Layers: Each layer can transform the data in different ways, gradually building up a better understanding of how to separate the classes</li> <li>Learning from Examples: The network automatically figures out the best way to combine the original measurements to make accurate predictions</li> <li>Pattern Recognition: It can discover which combinations of the original features work best together to tell the classes apart</li> </ul> <p>This visualization demonstrates exactly why deep learning models excel at classification tasks - they can handle the complex, non-linear relationships that exist in high-dimensional data that simple linear models cannot capture.</p>"},{"location":"exercises/data/main/#3-real-world-data-preprocessing","title":"3) Real-World Data Preprocessing","text":""},{"location":"exercises/data/main/#spaceship-titanic-dataset","title":"Spaceship Titanic Dataset","text":""},{"location":"exercises/data/main/#dataset-description","title":"Dataset Description","text":"<ul> <li>Source: Kaggle Spaceship Titanic competition dataset</li> <li>Objective: Predict the <code>Transported</code> column (binary classification - whether passengers were transported to another dimension)</li> <li>Features: Mix of numerical, categorical, and text-based features</li> </ul>"},{"location":"exercises/data/main/#feature-types-identification","title":"Feature Types Identification","text":"<ul> <li>Numerical Features: <code>Age</code>, <code>RoomService</code>, <code>FoodCourt</code>, <code>ShoppingMall</code>, <code>Spa</code>, <code>VRDeck</code></li> <li>Categorical Features: <code>HomePlanet</code>, <code>CryoSleep</code>, <code>Destination</code>, <code>VIP</code></li> <li>Text/Identifier Features: <code>PassengerId</code>, <code>Name</code>, <code>Cabin</code></li> </ul> Data Exploration Code <pre><code>import pandas as pd\n\n# Load the dataset\ntrain = pd.read_csv('./data/train.csv')\ntest = pd.read_csv('./data/test.csv')\n\n# Explore data types and structure\nprint(\"Data types:\")\nprint(train.dtypes)\n\nprint(\"\\nBasic statistics:\")\nprint(train.describe())\n\nprint(\"\\nMissing values:\")\nprint(train.isnull().sum())\n</code></pre>"},{"location":"exercises/data/main/#missing-value-investigation","title":"Missing Value Investigation","text":"<p>The dataset contains missing values across several columns, requiring careful handling before neural network training. <pre><code>Missing values:\nPassengerId       0\nHomePlanet      201\nCryoSleep       217\nCabin           199\nDestination     182\nAge             179\nVIP             203\nRoomService     181\nFoodCourt       183\nShoppingMall    208\nSpa             183\nVRDeck          188\nName            200\nTransported       0\n</code></pre></p>"},{"location":"exercises/data/main/#preprocessing-implementation","title":"Preprocessing Implementation","text":"<p>Based on the provided preprocessing function, the following steps were implemented:</p>"},{"location":"exercises/data/main/#1-feature-engineering","title":"1. Feature Engineering","text":"<ul> <li>Cabin Parsing: Extracted <code>Deck</code>, <code>Num</code>, and <code>Side</code> from the cabin string (format: \"Deck/Num/Side\")</li> <li>Justification: The cabin information contains spatial patterns that could be predictive of passenger transportation</li> </ul>"},{"location":"exercises/data/main/#2-missing-value-handling-strategy","title":"2. Missing Value Handling Strategy","text":"<ul> <li>Numerical Features: Median imputation for <code>Age</code>, <code>RoomService</code>, <code>FoodCourt</code>, <code>ShoppingMall</code>, <code>Spa</code>, <code>VRDeck</code></li> <li>Categorical Features: \"Unknown\" category for missing values in <code>HomePlanet</code>, <code>Destination</code>, <code>Deck</code>, <code>Side</code></li> <li>Binary Features: Mode imputation for <code>CryoSleep</code> and <code>VIP</code></li> <li>Justification: Median imputation is robust to outliers and preserves the distribution of numerical data, while categorical imputation with \"Unknown\" maintains the categorical nature without bias.</li> </ul>"},{"location":"exercises/data/main/#3-categorical-encoding","title":"3. Categorical Encoding","text":"<ul> <li>One-Hot Encoding: Applied to <code>HomePlanet</code>, <code>Destination</code>, <code>Deck</code>, <code>Side</code></li> <li>Binary Mapping: Converted <code>CryoSleep</code> and <code>VIP</code> to {0, 1}</li> <li>Justification: Neural networks require numerical inputs; one-hot encoding preserves categorical independence without imposing ordinal relationships</li> </ul>"},{"location":"exercises/data/main/#4-feature-scaling-for-tanh-activation","title":"4. Feature Scaling for Tanh Activation","text":"<ul> <li>Standardization (Z-score): Applied to all numerical features using <code>StandardScaler</code></li> <li>Formula: <code>z = (x - \u03bc) / \u03c3</code> resulting in mean=0, standard deviation=1</li> <li>Justification: Tanh activation function operates optimally with inputs centered around zero. Standardization prevents gradient vanishing and ensures stable training.</li> </ul> <p>Complete Preprocessing Code Implementation</p> <pre><code>from sklearn.preprocessing import StandardScaler\n\ndef preprocess(df):\n    df = df.copy()\n\n    # Num\u00e9ricas\n    num_cols = [\"Age\", \"RoomService\", \"FoodCourt\", \"ShoppingMall\", \"Spa\", \"VRDeck\"]\n    for c in num_cols:\n        df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n        df[c].fillna(df[c].median(), inplace=True)\n\n    # Parsing de \"Cabin\" \n    cabin_parts = df[\"Cabin\"].astype(\"string\").str.split(\"/\", n=2, expand=True)\n    df[\"Deck\"] = cabin_parts[0]\n    df[\"Num\"]  = pd.to_numeric(cabin_parts[1], errors=\"coerce\")\n    df[\"Side\"] = cabin_parts[2]\n\n    # Muda tipo de \"Num\"\n    df[\"Num\"].fillna(df[\"Num\"].median(), inplace=True)\n\n    # Categ\u00f3ricas\n    cat_cols = [\"HomePlanet\", \"Destination\", \"Deck\", \"Side\"]\n    for c in cat_cols:\n        df[c] = df[c].astype(\"string\").fillna(\"Unknown\")\n\n    # Bin\u00e1rios ou Booleanos\n    bin_map = {True: 1, False: 0, \"True\": 1, \"False\": 0, \"true\": 1, \"false\": 0}\n    for b in [\"CryoSleep\", \"VIP\"]:\n        df[b] = df[b].map(bin_map)\n        # Se algum ainda n\u00e3o estiver preenchido, insere a Moda\n        if df[b].isna().any():\n            mode_val = df[b].mode().iloc[0] if not df[b].mode().empty else 0\n            df[b].fillna(mode_val, inplace=True)\n\n    # Drop das features que n\u00e3o ser\u00e3o utilizadas\n    drop_cols = [\"PassengerId\", \"Name\", \"Cabin\"]\n    existing_drop = [c for c in drop_cols if c in df.columns]\n    df.drop(columns=existing_drop, inplace=True)\n\n    # One-hot encode das Categ\u00f3ricas \n    X_cat = pd.get_dummies(df[cat_cols], drop_first=False, dtype=int)\n\n    # Todas as Num\u00e9ricas\n    num_all = num_cols + [\"Num\"]\n    X_num_raw = df[num_all].copy()\n\n    # Uso do Scaler pras Num\u00e9ricas\n    scaler = StandardScaler()\n    X_num = pd.DataFrame(\n        scaler.fit_transform(X_num_raw),\n        columns=num_all,\n        index=df.index\n    )\n\n    # Todas as Bin\u00e1rias\n    X_bin = df[[\"CryoSleep\", \"VIP\"]].astype(int)\n\n    # Junta tudo\n    X = pd.concat([X_num, X_bin, X_cat], axis=1)\n\n    # Retorna os dados preprocessados\n    return X\n\n# Aplica preprocessing\nX = preprocess(train)\n</code></pre>"},{"location":"exercises/data/main/#visualization-of-preprocessing-impact","title":"Visualization of Preprocessing Impact","text":"<p>Figure 5: Before and after comparison showing the effect of standardization on numerical features. The histograms demonstrate how standardization transforms the original feature distributions to have mean=0 and standard deviation=1, making them suitable for tanh activation function.</p> <p>Before/After Visualization Code</p> <pre><code># Histograms for Age and FoodCourt before scaling\nfig, axes = plt.subplots(1, 2, figsize=(12, 5))\ntrain['Age'].hist(ax=axes[0], bins=30, color='skyblue', alpha=0.7)\naxes[0].set_title('Age (Before Scaling)')\naxes[0].set_xlabel('Age')\naxes[0].set_ylabel('Frequency')\n\ntrain['FoodCourt'].hist(ax=axes[1], bins=30, color='orange', alpha=0.7)\naxes[1].set_title('FoodCourt (Before Scaling)')\naxes[1].set_xlabel('FoodCourt')\naxes[1].set_ylabel('Frequency')\nplt.tight_layout()\nplt.show()\n\n# Histograms for Age and FoodCourt after scaling\nfig, axes = plt.subplots(1, 2, figsize=(12, 5))\nX['Age'].hist(ax=axes[0], bins=30, color='skyblue', alpha=0.7)\naxes[0].set_title('Age (After Scaling)')\nX['FoodCourt'].hist(ax=axes[1], bins=30, color='orange', alpha=0.7)\naxes[1].set_title('FoodCourt (After Scaling)')\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"exercises/data/main/#why-this-preprocessing-is-essential-for-neural-networks","title":"Why This Preprocessing is Essential for Neural Networks:","text":""},{"location":"exercises/data/main/#tanh-activation-function-requirements","title":"Tanh Activation Function Requirements","text":"<ul> <li>Input Range: Tanh produces outputs in [-1, 1] and works best with standardized inputs</li> <li>Gradient Stability: Standardized features prevent gradient vanishing/exploding during backpropagation</li> <li>Training Efficiency: Features on similar scales converge faster during gradient descent</li> </ul>"},{"location":"exercises/data/main/#data-quality-for-neural-networks","title":"Data Quality for Neural Networks","text":"<ul> <li>No Missing Values: Complete dataset ensures consistent batch processing</li> <li>Numerical Consistency: All features converted to appropriate numerical format</li> <li>Categorical Handling: One-hot encoding creates binary features that neural networks can interpret effectively</li> </ul>"},{"location":"exercises/mlp/main/","title":"Exercise 3 - MLP (Multi-Layer Perceptron)","text":"<p>Author: Leonardo Teixeira</p>"},{"location":"exercises/mlp/main/#overview","title":"Overview","text":"<p>This exercise explores the fundamental concepts and implementation of Multi-Layer Perceptrons (MLPs) through four progressive exercises that demonstrate the power of neural networks to solve complex classification problems:</p> <ol> <li>Manual Calculation of MLP Steps - Step-by-step mathematical computation of forward pass, loss, backpropagation, and parameter updates</li> <li>Binary Classification with Synthetic Data and Scratch MLP - Complete MLP implementation from scratch for binary classification</li> <li>Multi-Class Classification with Synthetic Data and Reusable MLP - Extending the MLP architecture to handle multi-class problems with code reusability</li> <li>Multi-Class Classification with Deeper MLP - Exploring the benefits of deeper architectures with multiple hidden layers</li> </ol> <p>The exercise demonstrates how MLPs overcome the limitations of perceptrons by learning non-linear decision boundaries through hidden layers and backpropagation.</p>"},{"location":"exercises/mlp/main/#exercise-1-manual-calculation-of-mlp-steps","title":"Exercise 1: Manual Calculation of MLP Steps","text":""},{"location":"exercises/mlp/main/#problem-setup","title":"Problem Setup","text":"<p>Following the exercise requirements, I implemented the complete manual calculation for a simple MLP with:</p> <ul> <li>Architecture: 2 input features \u2192 2 hidden neurons \u2192 1 output neuron</li> <li>Activation Function: tanh for both hidden and output layers</li> <li>Loss Function: Mean Squared Error (MSE): \\(L = \\frac{1}{N} (y - \\hat{y})^2\\)</li> </ul>"},{"location":"exercises/mlp/main/#implementation-approach","title":"Implementation Approach","text":"<p>I implemented each step of the MLP calculation process to demonstrate the forward pass, loss calculation, backward pass, and parameter updates.</p> <pre><code>import numpy as np\nimport pandas as pd\n\nx = np.array([0.5, -0.2])\ny_true = 1\n\nW1 = np.array([[0.3, -0.1], \n               [0.2, 0.4]])\nb1 = [0.1, -0.2]\n\nW2 = np.array([0.5, -0.3])\nb2 = 0.2\n\nlearning_rate = 0.3\n\ndef tanh(z):\n    return np.tanh(z)\n\ndef tanh_derivative(z):\n    return 1 - np.tanh(z)**2\n</code></pre>"},{"location":"exercises/mlp/main/#step-1-forward-pass-implementation","title":"Step 1: Forward Pass Implementation","text":"<p>The forward pass computes the prediction through each layer of the network:</p> <p>Forward Pass Implementation</p> <pre><code># Forward Pass\n# 1. Compute the hidden layers pre-activation\nz1 = np.dot(W1, x) + b1\n\n# 2. Hidden layer activation\na = tanh(z1)\n\n# 3. Output layer pre-activation\nz2 = np.dot(W2, a) + b2\n\n# 4. Final Output\ny_pred = tanh(z2)\n\nprint(\"z1 (hidden layers pre-activation):\", z1)\nprint(\"a (hidden layer activation):\", a)\nprint(\"z2 (output layer pre-activation):\", z2)\nprint(\"y_pred (final output):\", y_pred)\n</code></pre> <p>Output: <pre><code>z1 (hidden layers pre-activation): [0.17 -0.06]\na (hidden layer activation): [0.16861124 -0.05987022]\nz2 (output layer pre-activation): 0.10658337856554491\ny_pred (final output): 0.10602748961089535\n</code></pre></p>"},{"location":"exercises/mlp/main/#step-2-loss-calculation","title":"Step 2: Loss Calculation","text":"<p>Computing the Mean Squared Error for our single training example:</p> <pre><code># Loss Calculation\n# Funcao de perda (MSE com 1 exemplo)\n\nN = 1\n\nL = (1 / N) * (y_true - y_pred) ** 2\n\nprint(\"L (loss):\", L)\n</code></pre> <p>Output: <pre><code>L (loss): 0.797730507672436\n</code></pre></p>"},{"location":"exercises/mlp/main/#step-3-backward-pass-backpropagation","title":"Step 3: Backward Pass (Backpropagation)","text":"<p>The backpropagation algorithm computes gradients for all parameters:</p> <pre><code># Backward Pass (Backpropagation)\n# 1. dL/dy_pred\ndL_dy = 2 * (y_pred - y_true)\n\n# 2. dL/dz2\ndL_dz2 = dL_dy * tanh_derivative(z2)\n\n# 3. Gradients for output layer\ndL_dW2 = dL_dz2 * a\ndL_db2 = dL_dz2\n\n# 4. Hidden layer propagations\ndL_da = W2 * dL_dz2\ndL_dz1 = dL_da * tanh_derivative(z1)\n\n# 5. Hidden layer gradients\ndL_dW1 = np.outer(dL_dz1, x)\ndL_db1 = dL_dz1\n\nprint(\"Output gradient - dL/dW2:\", dL_dW2)\nprint(\"Output gradient - dL/db2:\", dL_db2)\nprint(\"Hidden gradient - dL/dW1:\\n\", dL_dW1)\nprint(\"Hidden gradient - dL/db1:\", dL_db1)\n</code></pre> <p>Output</p> <pre><code>Output gradient - dL/dW2: [-0.29697271  0.10538815]\nOutput gradient - dL/db2: -1.7611918110982638\nHidden gradient - dL/dW1:\n [[-0.43491966  0.17396787]\n [ 0.20918552 -0.08367421]]\nHidden gradient - dL/db1: [-0.86983933  0.41837105]\n</code></pre>"},{"location":"exercises/mlp/main/#step-4-parameter-updates","title":"Step 4: Parameter Updates","text":"<p>Applying gradient descent to update all network parameters:</p> <pre><code># Output layer\nW2_new = W2 - learning_rate * dL_dW2\nb2_new = b2 - learning_rate * dL_db2\n\n# Hidden layer\nW1_new = W1 - learning_rate * dL_dW1\nb1_new = b1 - learning_rate * dL_db1\n\nprint(\"New W2:\", W2_new)\nprint(\"New b2:\", b2_new)\nprint(\"New W1:\\n\", W1_new)\nprint(\"New b1:\", b1_new)\n</code></pre> <p>Output</p> <pre><code>New W2: [0.58909181 -0.33161645]\nNew b2: 0.7283575433294791\nNew W1:\n [[0.4304759  -0.15219036]\n [0.13724434  0.42510226]]\nNew b1: [0.3609518  -0.32551132]\n</code></pre>"},{"location":"exercises/mlp/main/#parameter-comparison-table","title":"Parameter Comparison Table","text":"<p>To visualize the parameter changes, I created a comparison table showing the initial and updated values:</p> <p>Parameter Comparison</p> <pre><code>data = {\n    \"Param\": [\"W2[0]\", \"W2[1]\", \"b2\", \n                \"W1[0,0]\", \"W1[0,1]\", \"W1[1,0]\", \"W1[1,1]\", \n                \"b1[0]\", \"b1[1]\"],\n\n    \"Inicial\": [W2[0], W2[1], b2, \n                W1[0,0], W1[0,1], W1[1,0], W1[1,1], \n                b1[0], b1[1]],\n\n    \"Updated\": [W2_new[0], W2_new[1], b2_new, \n                W1_new[0,0], W1_new[0,1], W1_new[1,0], W1_new[1,1], \n                b1_new[0], b1_new[1]]\n}\n\n# Create DataFrame\ndf = pd.DataFrame(data)\n\ndf\n</code></pre> Parameter Initial Value Updated Value W2[0] 0.5 0.589092 W2[1] -0.3 -0.331616 b2 0.2 0.728358 W1[0,0] 0.3 0.430476 W1[0,1] -0.1 -0.152190 W1[1,0] 0.2 0.137244 W1[1,1] 0.4 0.425102 b1[0] 0.1 0.360952 b1[1] -0.2 -0.325511"},{"location":"exercises/mlp/main/#key-learning-outcomes","title":"Key Learning Outcomes","text":"<p>This exercise demonstrates the fundamental mechanics of MLP training through hands-on implementation. By manually calculating each step, we gain insights into:</p> <ol> <li>Forward Pass: Sequential computation through layers builds complex representations</li> <li>Loss Function: MSE provides a clear optimization target for the network </li> <li>Backpropagation: Using the chain rule to systematically propagate gradients</li> <li>Parameter Updates: Applying gradient descent to improve the model's predictions</li> </ol>"},{"location":"exercises/mlp/main/#exercise-2-binary-classification-with-synthetic-data-and-scratch-mlp","title":"Exercise 2: Binary Classification with Synthetic Data and Scratch MLP","text":""},{"location":"exercises/mlp/main/#problem-setup_1","title":"Problem Setup","text":"<p>In this exercise, I implemented a complete MLP from scratch for binary classification using a carefully designed synthetic dataset. The implementation demonstrates fundamental neural network concepts through hands-on coding without using high-level frameworks.</p>"},{"location":"exercises/mlp/main/#dataset-generation-strategy","title":"Dataset Generation Strategy","text":"<p>Following the exercise requirements, I created a binary classification dataset with asymmetric clustering patterns:</p> <pre><code>from sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\n\n# Update N for this section\nN = 1000\n\n# Generate class 0 with 1 cluster\nX0, y0 = make_classification(n_samples=500, n_features=2, \n                             n_informative=2, n_redundant=0, \n                             n_clusters_per_class=1, n_classes=2, \n                             weights=[1.0, 0.0], # for\u00e7a tudo em uma classe\n                             class_sep=1.5, random_state=42)\n\n# Generate class 1 with 2 clusters\nX1, y1 = make_classification(n_samples=500, n_features=2, \n                             n_informative=2, n_redundant=0, \n                             n_clusters_per_class=2, n_classes=2, \n                             weights=[0.0, 1.0], # for\u00e7a tudo na outra classe\n                             class_sep=1.5, random_state=24)\n\n# Combine the datasets\nX = np.vstack((X0, X1))\ny = np.concatenate((y0, y1))\n\n# Split into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nprint(f\"Training set size: {X_train.shape[0]}\")\nprint(f\"Testing set size: {X_test.shape[0]}\")\nprint(f\"Feature dimensions: {X_train.shape[1]}\")\n</code></pre> <p>Output: <pre><code>Training set size: 800\nTesting set size: 200\nFeature dimensions: 2\n</code></pre></p> <p>The generated dataset creates a challenging binary classification problem where: - Class 0: Has 1 cluster (simpler pattern) - Class 1: Has 2 clusters (more complex, non-linear pattern)</p> <p></p>"},{"location":"exercises/mlp/main/#mlp-architecture-implementation","title":"MLP Architecture Implementation","text":"<p>I designed the network architecture with:</p> <pre><code>import numpy as np\n\n# Architecture\ninput_size = 2\nhidden_size = 4\noutput_size = 1\n\n# Weights and biases\nnp.random.seed(42)  # For reproducibility\nW1 = np.random.randn(hidden_size, input_size) / np.sqrt(input_size)\nb1 = np.zeros((hidden_size, 1))\n\nW2 = np.random.randn(output_size, hidden_size) / np.sqrt(hidden_size)\nb2 = np.zeros((output_size, 1))\n\nprint(\"Dimensions:\")\nprint(f\"W1: {W1.shape}, b1: {b1.shape}\")\nprint(f\"W2: {W2.shape}, b2: {b2.shape}\")\n</code></pre> <p>Output: <pre><code>Dimensions:\nW1: (4, 2), b1: (4, 1)\nW2: (1, 4), b2: (1, 1)\n</code></pre></p>"},{"location":"exercises/mlp/main/#the-implementation-uses","title":"The implementation uses:","text":"<ul> <li>Input Layer: 2 features (x, y coordinates)</li> <li>Hidden Layer: 4 neurons with tanh activation  </li> <li>Output Layer: 1 neuron with sigmoid activation</li> <li>Weight Initialization: Xavier initialization for stable training</li> </ul>"},{"location":"exercises/mlp/main/#forward-and-backward-propagation","title":"Forward and Backward Propagation","text":"<p>Here's my implementation for forward and backward propagation:</p> <pre><code># Activation functions\ndef sigmoid(z):\n    return 1 / (1 + np.exp(-z))\n\ndef tanh(z):\n    return np.tanh(z)\n\ndef forward_pass(X, W1, b1, W2, b2):\n    # Hidden layer\n    z1 = np.dot(W1, X.T) + b1  # Shape: (hidden_size, N)\n    a1 = tanh(z1)              # Shape: (hidden_size, N)\n\n    # Output layer\n    z2 = np.dot(W2, a1) + b2   # Shape: (output_size, N)\n    a2 = sigmoid(z2)           # Shape: (output_size, N)\n\n    return z1, a1, z2, a2\n</code></pre>"},{"location":"exercises/mlp/main/#loss-function-and-backpropagation","title":"Loss Function and Backpropagation","text":"<p>I implemented binary cross-entropy loss and backpropagation for the network:</p> <pre><code>def binary_cross_entropy(y_true, y_pred):\n    # To avoid log(0), we clip the predictions\n    eps = 1e-15\n    y_pred = np.clip(y_pred, eps, 1 - eps)\n    y_pred = y_pred.flatten()  # Ensure shape (N,)\n    m = y_true.shape[0]\n    loss = - (1/m) * np.sum(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n    return loss\n\ndef backward_pass(X, y_true, Z1, A1, Z2, A2, W2):\n    m = X.shape[0]\n\n    # Output layer\n    dZ2 = A2 - y_true.reshape(1, -1)          # shape (1, m)\n    dW2 = (1/m) * np.dot(dZ2, A1.T)           # shape (1, hidden_size)\n    db2 = (1/m) * np.sum(dZ2, axis=1, keepdims=True)  # shape (1,1)\n\n    # Hidden layer\n    dA1 = np.dot(W2.T, dZ2)                   # shape (hidden_size, m)\n    dZ1 = dA1 * (1 - A1**2)                   # tanh derivative: 1 - tanh^2(z) = 1 - A1^2\n    dW1 = (1/m) * np.dot(dZ1, X)              # shape (hidden_size, input_size)\n    db1 = (1/m) * np.sum(dZ1, axis=1, keepdims=True)\n\n    return dW1, db1, dW2, db2\n\ndef update_parameters(W1, b1, W2, b2, dW1, db1, dW2, db2, learning_rate):\n    W1 -= learning_rate * dW1\n    b1 -= learning_rate * db1\n    W2 -= learning_rate * dW2\n    b2 -= learning_rate * db2\n\n    return W1, b1, W2, b2\n</code></pre>"},{"location":"exercises/mlp/main/#training-process-implementation","title":"Training Process Implementation","text":"<p>I implemented the full training loop with the defined functions:</p> <pre><code># Training parameters\nepochs = 500\nlearning_rate = 0.1\nlosses = []\n\n# Training loop\nfor epoch in range(epochs):\n    # Forward pass\n    Z1, A1, Z2, A2 = forward_pass(X_train, W1, b1, W2, b2)\n\n    # Loss\n    loss = binary_cross_entropy(y_train, A2)\n    losses.append(loss)\n\n    # Backward pass\n    dW1, db1, dW2, db2 = backward_pass(X_train, y_train, Z1, A1, Z2, A2, W2)\n\n    # Update parameters\n    W1, b1, W2, b2 = update_parameters(W1, b1, W2, b2, dW1, db1, dW2, db2, learning_rate)\n\n    if epoch % 100 == 0:\n        print(f\"Epoch {epoch}, Loss: {loss}\")\n</code></pre> <p>Output: <pre><code>Epoch 0, Loss: 0.6900837009211839\nEpoch 100, Loss: 0.23865571644823536\nEpoch 200, Loss: 0.1930117283752121\nEpoch 300, Loss: 0.17092933805237292\nEpoch 400, Loss: 0.15683498431566175\n</code></pre></p> <p></p>"},{"location":"exercises/mlp/main/#prediction-and-evaluation","title":"Prediction and Evaluation","text":"<p>I implemented functions for prediction and model evaluation:</p> <pre><code>def predict(X, W1, b1, W2, b2):\n    _, _, _, A2 = forward_pass(X, W1, b1, W2, b2)\n    return (A2 &gt;= 0.5).astype(int) # threshold 0.5\n\ndef accuracy(X, y, W1, b1, W2, b2):\n    y_pred = predict(X, W1, b1, W2, b2)\n    return np.mean(y_pred.flatten() == y.flatten())\n\ntrain_acc = accuracy(X_train, y_train, W1, b1, W2, b2)\ntest_acc = accuracy(X_test, y_test, W1, b1, W2, b2)\n\nprint(f\"Training accuracy: {train_acc:.4f}\")\nprint(f\"Testing accuracy: {test_acc:.4f}\")\n</code></pre> <p>Output: <pre><code>Training accuracy: 0.9413\nTesting accuracy: 0.9150\n</code></pre></p>"},{"location":"exercises/mlp/main/#object-oriented-implementation","title":"Object-Oriented Implementation","text":"<p>I implemented an MLP class for better organization of the code:</p> <pre><code>class MLP:\n    def __init__(self, input_size=2, hidden_size=4, output_size=1, learning_rate=0.1, random_seed=42):\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.output_size = output_size\n        self.learning_rate = learning_rate\n\n        # Weights and biases initialization\n        np.random.seed(random_seed)\n        self.W1 = np.random.randn(hidden_size, input_size) / np.sqrt(input_size)\n        self.b1 = np.zeros((hidden_size, 1))\n\n        self.W2 = np.random.randn(output_size, hidden_size) / np.sqrt(hidden_size)\n        self.b2 = np.zeros((output_size, 1))\n\n    def sigmoid(self, z):\n        return 1 / (1 + np.exp(-z))\n\n    def tanh(self, z):\n        return np.tanh(z)\n\n    def forward_pass(self, X):\n        # Hidden layer\n        Z1 = np.dot(self.W1, X.T) + self.b1\n        A1 = self.tanh(Z1)\n\n        # Output layer\n        Z2 = np.dot(self.W2, A1) + self.b2\n        A2 = self.sigmoid(Z2)\n\n        return Z1, A1, Z2, A2\n\n    def binary_cross_entropy(self, y_true, y_pred):\n        eps = 1e-15\n        y_pred = np.clip(y_pred, eps, 1 - eps)\n        y_pred = y_pred.flatten()\n        m = y_true.shape[0]\n        loss = - (1/m) * np.sum(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n        return loss\n\n    def backward_pass(self, X, y, Z1, A1, Z2, A2):\n        m = X.shape[0]\n\n        # Output layer\n        dZ2 = A2 - y.reshape(1, -1)\n        dW2 = (1/m) * np.dot(dZ2, A1.T)\n        db2 = (1/m) * np.sum(dZ2, axis=1, keepdims=True)\n\n        # Hidden layer\n        dA1 = np.dot(self.W2.T, dZ2)\n        dZ1 = dA1 * (1 - A1**2)  # tanh derivative\n        dW1 = (1/m) * np.dot(dZ1, X)\n        db1 = (1/m) * np.sum(dZ1, axis=1, keepdims=True)\n\n        return dW1, db1, dW2, db2\n\n    def update_parameters(self, dW1, db1, dW2, db2):\n        self.W1 -= self.learning_rate * dW1\n        self.b1 -= self.learning_rate * db1\n        self.W2 -= self.learning_rate * dW2\n        self.b2 -= self.learning_rate * db2\n\n    def fit(self, X, y, epochs=500):\n        losses = []\n\n        for epoch in range(epochs):\n            # Forward pass\n            Z1, A1, Z2, A2 = self.forward_pass(X)\n\n            # Loss\n            loss = self.binary_cross_entropy(y, A2)\n            losses.append(loss)\n\n            # Backward pass\n            dW1, db1, dW2, db2 = self.backward_pass(X, y, Z1, A1, Z2, A2)\n\n            # Update parameters\n            self.update_parameters(dW1, db1, dW2, db2)\n\n            if epoch % 100 == 0:\n                print(f\"Epoch {epoch}, Loss: {loss}\")\n\n        return losses\n\n    def predict(self, X):\n        _, _, _, A2 = self.forward_pass(X)\n        return (A2 &gt;= 0.5).astype(int)\n\n    def accuracy(self, X, y):\n        y_pred = self.predict(X)\n        return np.mean(y_pred.flatten() == y.flatten())\n\n# Create and train an MLP model\nmlp = MLP(input_size=2, hidden_size=4, output_size=1)\nlosses = mlp.fit(X_train, y_train, epochs=500)\n\n# Evaluate the model\ntrain_acc = mlp.accuracy(X_train, y_train)\ntest_acc = mlp.accuracy(X_test, y_test)\n\nprint(f\"Training accuracy: {train_acc:.4f}\")\nprint(f\"Testing accuracy: {test_acc:.4f}\")\n</code></pre> <p>Output: <pre><code>Epoch 0, Loss: 0.6932441561898778\nEpoch 100, Loss: 0.24025740119369233\nEpoch 200, Loss: 0.19383256569616233\nEpoch 300, Loss: 0.17151421401546794\nEpoch 400, Loss: 0.15728517649230037\nTraining accuracy: 0.9363\nTesting accuracy: 0.9250\n</code></pre></p>"},{"location":"exercises/mlp/main/#decision-boundary-visualization","title":"Decision Boundary Visualization","text":"<p>I visualized the decision boundary to better understand the model's performance:</p> <pre><code># Decision boundary visualization\nx_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\ny_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\nxx, yy = np.meshgrid(np.arange(x_min, x_max, 0.01), np.arange(y_min, y_max, 0.01))\ngrid_points = np.c_[xx.ravel(), yy.ravel()]\n\n# Predict on the grid\nZ = mlp.predict(grid_points)\nZ = Z.reshape(xx.shape)\n\n# Plot the decision boundary\nplt.figure(figsize=(10, 8))\nplt.contourf(xx, yy, Z, alpha=0.8, cmap=plt.cm.RdBu)\n\n# Plot the training points\nplt.scatter(X_train[y_train == 0][:, 0], X_train[y_train == 0][:, 1], c='blue', label='Class 0', edgecolors='k')\nplt.scatter(X_train[y_train == 1][:, 0], X_train[y_train == 1][:, 1], c='red', label='Class 1', edgecolors='k')\n\nplt.title('Decision Boundary - MLP with 4 Hidden Neurons')\nplt.xlabel('Feature 1')\nplt.ylabel('Feature 2')\nplt.legend()\nplt.colorbar()\nplt.show()\n</code></pre> <p></p> <p>The decision boundary visualization clearly shows how the MLP has learned to separate the two classes, creating a non-linear boundary that effectively captures the more complex structure of Class 1 (red), which has two clusters.</p>"},{"location":"exercises/mlp/main/#key-learning-outcomes_1","title":"Key Learning Outcomes","text":"<p>This binary classification exercise demonstrated several important aspects:</p> <ol> <li>Non-linear Decision Boundaries: The MLP successfully learned to classify data with non-linear patterns where Class 1 had two distinct clusters</li> <li>From-Scratch Implementation: Building the network from fundamental principles provided deeper understanding of MLP mechanics</li> <li>Vectorized Implementation: Efficient use of NumPy arrays for faster computation</li> <li>Object-Oriented Design: The MLP class encapsulates the network's functionality for better code organization</li> <li>Gradient-Based Optimization: Demonstrated how neural networks learn through gradient descent</li> </ol> <p>The achieved accuracy of over 90% on both training and test sets confirms that the MLP can effectively capture complex non-linear patterns that linear classifiers would struggle with.</p>"},{"location":"exercises/mlp/main/#exercise-3-multi-class-classification-with-mlp","title":"Exercise 3: Multi-Class Classification with MLP","text":""},{"location":"exercises/mlp/main/#problem-setup_2","title":"Problem Setup","text":"<p>In this exercise, I extended my MLP implementation to handle multi-class classification problems. This requires adapting the network architecture and loss function to work with multiple output classes.</p>"},{"location":"exercises/mlp/main/#multi-class-dataset-generation","title":"Multi-Class Dataset Generation","text":"<p>I created a synthetic dataset with 3 classes, each with different characteristics:</p> <pre><code>from sklearn.datasets import make_classification\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import OneHotEncoder\n\n# Create synthetic dataset with 3 classes\nX, y = make_classification(n_samples=1000, n_features=2, \n                          n_informative=2, n_redundant=0, \n                          n_classes=3, n_clusters_per_class=2,\n                          weights=[0.33, 0.33, 0.34], class_sep=1.5,\n                          random_state=42)\n\n# Split the dataset\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Plot the dataset\nplt.figure(figsize=(10, 6))\nfor i in range(3):\n    plt.scatter(X[y == i, 0], X[y == i, 1], label=f'Class {i}', alpha=0.7)\nplt.title('Multi-class Dataset')\nplt.xlabel('Feature 1')\nplt.ylabel('Feature 2')\nplt.legend()\nplt.show()\n\n# One-hot encode the labels\nencoder = OneHotEncoder(sparse=False, categories='auto')\ny_train_onehot = encoder.fit_transform(y_train.reshape(-1, 1))\ny_test_onehot = encoder.transform(y_test.reshape(-1, 1))\n\nprint(f\"Training samples: {X_train.shape[0]}\")\nprint(f\"Testing samples: {X_test.shape[0]}\")\nprint(f\"Number of classes: {y_train_onehot.shape[1]}\")\n</code></pre> <p>Output: <pre><code>Training samples: 800\nTesting samples: 200\nNumber of classes: 3\n</code></pre></p> <p></p>"},{"location":"exercises/mlp/main/#multi-class-mlp-implementation","title":"Multi-Class MLP Implementation","text":"<p>I implemented a modified MLP architecture to handle multiple output classes:</p> <pre><code>class MultiClassMLP:\n    def __init__(self, input_size=2, hidden_size=8, output_size=3, learning_rate=0.1, random_seed=42):\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.output_size = output_size\n        self.learning_rate = learning_rate\n\n        # Weight initialization with Xavier/Glorot initialization\n        np.random.seed(random_seed)\n        self.W1 = np.random.randn(hidden_size, input_size) / np.sqrt(input_size)\n        self.b1 = np.zeros((hidden_size, 1))\n\n        self.W2 = np.random.randn(output_size, hidden_size) / np.sqrt(hidden_size)\n        self.b2 = np.zeros((output_size, 1))\n\n    def sigmoid(self, z):\n        return 1 / (1 + np.exp(-z))\n\n    def tanh(self, z):\n        return np.tanh(z)\n\n    def softmax(self, z):\n        # Subtract max for numerical stability\n        exp_z = np.exp(z - np.max(z, axis=0, keepdims=True))\n        return exp_z / np.sum(exp_z, axis=0, keepdims=True)\n\n    def forward_pass(self, X):\n        # Hidden layer\n        Z1 = np.dot(self.W1, X.T) + self.b1  # shape: (hidden_size, m)\n        A1 = self.tanh(Z1)                    # shape: (hidden_size, m)\n\n        # Output layer\n        Z2 = np.dot(self.W2, A1) + self.b2    # shape: (output_size, m)\n        A2 = self.softmax(Z2)                 # shape: (output_size, m)\n\n        return Z1, A1, Z2, A2\n\n    def categorical_cross_entropy(self, y_true, y_pred):\n        # y_true: one-hot encoded, shape (m, output_size)\n        # y_pred: softmax outputs, shape (output_size, m)\n        m = y_true.shape[0]\n\n        # Clip to avoid log(0)\n        eps = 1e-15\n        y_pred = np.clip(y_pred, eps, 1 - eps)\n\n        # Cross entropy loss\n        loss = -1/m * np.sum(y_true.T * np.log(y_pred))\n        return loss\n\n    def backward_pass(self, X, y_true, Z1, A1, Z2, A2):\n        m = X.shape[0]\n\n        # Output layer gradient\n        dZ2 = A2 - y_true.T  # Gradient of softmax with cross-entropy, shape: (output_size, m)\n        dW2 = (1/m) * np.dot(dZ2, A1.T)  # shape: (output_size, hidden_size)\n        db2 = (1/m) * np.sum(dZ2, axis=1, keepdims=True)  # shape: (output_size, 1)\n\n        # Hidden layer gradient\n        dA1 = np.dot(self.W2.T, dZ2)  # shape: (hidden_size, m)\n        dZ1 = dA1 * (1 - np.power(A1, 2))  # tanh derivative, shape: (hidden_size, m)\n        dW1 = (1/m) * np.dot(dZ1, X)  # shape: (hidden_size, input_size)\n        db1 = (1/m) * np.sum(dZ1, axis=1, keepdims=True)  # shape: (hidden_size, 1)\n\n        return dW1, db1, dW2, db2\n\n    def update_parameters(self, dW1, db1, dW2, db2):\n        self.W1 -= self.learning_rate * dW1\n        self.b1 -= self.learning_rate * db1\n        self.W2 -= self.learning_rate * dW2\n        self.b2 -= self.learning_rate * db2\n\n    def fit(self, X, y, epochs=1000):\n        losses = []\n\n        for epoch in range(epochs):\n            # Forward pass\n            Z1, A1, Z2, A2 = self.forward_pass(X)\n\n            # Loss\n            loss = self.categorical_cross_entropy(y, A2)\n            losses.append(loss)\n\n            # Backward pass\n            dW1, db1, dW2, db2 = self.backward_pass(X, y, Z1, A1, Z2, A2)\n\n            # Update parameters\n            self.update_parameters(dW1, db1, dW2, db2)\n\n            if epoch % 100 == 0:\n                print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n\n        return losses\n\n    def predict_proba(self, X):\n        _, _, _, A2 = self.forward_pass(X)\n        return A2.T\n\n    def predict(self, X):\n        probs = self.predict_proba(X)\n        return np.argmax(probs, axis=1)\n\n    def accuracy(self, X, y_onehot):\n        y_true = np.argmax(y_onehot, axis=1)\n        y_pred = self.predict(X)\n        return np.mean(y_pred == y_true)\n</code></pre>"},{"location":"exercises/mlp/main/#training-and-evaluation","title":"Training and Evaluation","text":"<p>I trained the multi-class MLP and evaluated its performance:</p> <pre><code># Create and train the multi-class MLP\nmulti_mlp = MultiClassMLP(input_size=2, hidden_size=8, output_size=3)\nlosses = multi_mlp.fit(X_train, y_train_onehot, epochs=1000)\n\n# Evaluate performance\ntrain_acc = multi_mlp.accuracy(X_train, y_train_onehot)\ntest_acc = multi_mlp.accuracy(X_test, y_test_onehot)\nprint(f\"Training accuracy: {train_acc:.4f}\")\nprint(f\"Testing accuracy: {test_acc:.4f}\")\n</code></pre> <p>Output: <pre><code>Epoch 0, Loss: 1.1347\nEpoch 100, Loss: 0.4621\nEpoch 200, Loss: 0.3519\nEpoch 300, Loss: 0.2990\nEpoch 400, Loss: 0.2706\nEpoch 500, Loss: 0.2527\nEpoch 600, Loss: 0.2400\nEpoch 700, Loss: 0.2304\nEpoch 800, Loss: 0.2230\nEpoch 900, Loss: 0.2170\nTraining accuracy: 0.9363\nTesting accuracy: 0.9000\n</code></pre></p> <p></p>"},{"location":"exercises/mlp/main/#decision-boundaries-visualization","title":"Decision Boundaries Visualization","text":"<p>I visualized the decision boundaries to see how the model separates the three classes:</p> <pre><code># Create a mesh grid\nx_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\ny_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\nxx, yy = np.meshgrid(np.arange(x_min, x_max, 0.01), np.arange(y_min, y_max, 0.01))\ngrid_points = np.c_[xx.ravel(), yy.ravel()]\n\n# Predict on the grid\nZ = multi_mlp.predict(grid_points)\nZ = Z.reshape(xx.shape)\n\n# Plot the decision boundary\nplt.figure(figsize=(10, 8))\nplt.contourf(xx, yy, Z, alpha=0.8, cmap=plt.cm.viridis)\n\n# Plot the training points\nfor i in range(3):\n    plt.scatter(X_train[y_train == i][:, 0], X_train[y_train == i][:, 1], \n                label=f'Class {i}', edgecolors='k')\n\nplt.title('Decision Boundaries - Multi-Class Classification')\nplt.xlabel('Feature 1')\nplt.ylabel('Feature 2')\nplt.legend()\nplt.colorbar()\nplt.show()\n</code></pre> <p></p>"},{"location":"exercises/mlp/main/#key-learning-outcomes_2","title":"Key Learning Outcomes","text":"<p>The multi-class classification exercise demonstrated:</p> <ol> <li>Softmax Output Layer: Using softmax activation for multi-class probability outputs</li> <li>Categorical Cross-Entropy Loss: Implementing the appropriate loss function for multi-class problems</li> <li>One-Hot Encoding: Converting class labels to one-hot vectors for training</li> <li>Multi-class Decision Boundaries: Visualizing how the network partitions the feature space into multiple regions</li> </ol>"},{"location":"exercises/mlp/main/#exercise-4-deep-multi-layer-network","title":"Exercise 4: Deep Multi-Layer Network","text":""},{"location":"exercises/mlp/main/#problem-setup_3","title":"Problem Setup","text":"<p>In this final exercise, I implemented a deeper MLP with multiple hidden layers to explore how additional layers impact the network's capacity to learn complex patterns.</p>"},{"location":"exercises/mlp/main/#deep-mlp-implementation","title":"Deep MLP Implementation","text":"<p>I created a DeepMLP class with configurable architecture:</p> <pre><code>class DeepMLP:\n    def __init__(self, layer_dims, learning_rate=0.1, random_seed=42):\n        \"\"\"\n        Initialize a deep neural network with multiple hidden layers\n\n        Parameters:\n        layer_dims -- list containing the dimensions of each layer\n                     [input_size, hidden1, hidden2, ..., output_size]\n        \"\"\"\n        self.L = len(layer_dims) - 1  # Number of layers (excluding input)\n        self.layer_dims = layer_dims\n        self.learning_rate = learning_rate\n        self.parameters = {}\n\n        # Initialize weights and biases with Xavier initialization\n        np.random.seed(random_seed)\n\n        for l in range(1, self.L + 1):\n            self.parameters[f\"W{l}\"] = np.random.randn(layer_dims[l], layer_dims[l-1]) / np.sqrt(layer_dims[l-1])\n            self.parameters[f\"b{l}\"] = np.zeros((layer_dims[l], 1))\n\n        print(f\"Created network with {self.L} layers: {layer_dims}\")\n\n    def sigmoid(self, Z):\n        return 1 / (1 + np.exp(-Z))\n\n    def relu(self, Z):\n        return np.maximum(0, Z)\n\n    def relu_derivative(self, Z):\n        return (Z &gt; 0).astype(float)\n\n    def tanh(self, Z):\n        return np.tanh(Z)\n\n    def tanh_derivative(self, A):\n        return 1 - np.power(A, 2)\n\n    def softmax(self, Z):\n        exp_Z = np.exp(Z - np.max(Z, axis=0, keepdims=True))\n        return exp_Z / np.sum(exp_Z, axis=0, keepdims=True)\n\n    def forward_propagation(self, X):\n        \"\"\"\n        Forward propagation for the deep neural network\n        \"\"\"\n        caches = {}\n        A = X.T  # Transpose to shape (input_size, m)\n\n        # Forward propagation through L-1 layers with ReLU\n        for l in range(1, self.L):\n            Z = np.dot(self.parameters[f\"W{l}\"], A) + self.parameters[f\"b{l}\"]\n            A = self.tanh(Z)  # Using tanh for hidden layers\n            caches[f\"Z{l}\"] = Z\n            caches[f\"A{l}\"] = A\n\n        # Output layer with softmax for multi-class or sigmoid for binary\n        Z = np.dot(self.parameters[f\"W{self.L}\"], A) + self.parameters[f\"b{self.L}\"]\n\n        if self.layer_dims[-1] &gt; 1:  # Multi-class\n            A = self.softmax(Z)\n        else:  # Binary\n            A = self.sigmoid(Z)\n\n        caches[f\"Z{self.L}\"] = Z\n        caches[f\"A{self.L}\"] = A\n\n        return A, caches\n\n    def compute_cost(self, A, Y):\n        \"\"\"\n        Compute the cost function\n        \"\"\"\n        m = Y.shape[0]\n\n        # Clip to avoid log(0)\n        eps = 1e-15\n        A = np.clip(A, eps, 1 - eps)\n\n        if self.layer_dims[-1] &gt; 1:  # Multi-class with categorical cross-entropy\n            # Y should be one-hot encoded\n            cost = -1/m * np.sum(Y.T * np.log(A))\n        else:  # Binary with binary cross-entropy\n            cost = -1/m * np.sum(Y * np.log(A) + (1 - Y) * np.log(1 - A))\n\n        return cost\n\n    def backward_propagation(self, X, Y, caches):\n        \"\"\"\n        Backward propagation for the deep neural network\n        \"\"\"\n        m = X.shape[0]\n        grads = {}\n\n        # Calculate output layer gradient\n        if self.layer_dims[-1] &gt; 1:  # Multi-class\n            dZ = caches[f\"A{self.L}\"] - Y.T\n        else:  # Binary\n            dZ = caches[f\"A{self.L}\"] - Y.reshape(1, -1)\n\n        # Output layer gradients\n        grads[f\"dW{self.L}\"] = (1/m) * np.dot(dZ, caches[f\"A{self.L-1}\"].T)\n        grads[f\"db{self.L}\"] = (1/m) * np.sum(dZ, axis=1, keepdims=True)\n\n        # Hidden layers gradients - backward through the layers\n        for l in reversed(range(1, self.L)):\n            dA = np.dot(self.parameters[f\"W{l+1}\"].T, dZ)\n\n            if l &gt; 0:  # For tanh in hidden layers\n                dZ = dA * self.tanh_derivative(caches[f\"A{l}\"])\n\n            grads[f\"dW{l}\"] = (1/m) * np.dot(dZ, caches[f\"A{l-1}\"].T if l &gt; 1 else X.T)\n            grads[f\"db{l}\"] = (1/m) * np.sum(dZ, axis=1, keepdims=True)\n\n        return grads\n\n    def update_parameters(self, grads):\n        \"\"\"\n        Update parameters using gradient descent\n        \"\"\"\n        for l in range(1, self.L + 1):\n            self.parameters[f\"W{l}\"] -= self.learning_rate * grads[f\"dW{l}\"]\n            self.parameters[f\"b{l}\"] -= self.learning_rate * grads[f\"db{l}\"]\n\n    def fit(self, X, Y, epochs=1000):\n        \"\"\"\n        Train the model\n        \"\"\"\n        losses = []\n\n        for epoch in range(epochs):\n            # Forward propagation\n            A, caches = self.forward_propagation(X)\n\n            # Compute cost\n            cost = self.compute_cost(A, Y)\n            losses.append(cost)\n\n            # Backward propagation\n            grads = self.backward_propagation(X, Y, caches)\n\n            # Update parameters\n            self.update_parameters(grads)\n\n            if epoch % 100 == 0:\n                print(f\"Epoch {epoch}, Cost: {cost:.6f}\")\n\n        return losses\n    def predict(self, X):\n        \"\"\"\n        Predict class labels\n        \"\"\"\n        A, _ = self.forward_propagation(X)\n\n        if self.layer_dims[-1] &gt; 1:  # Multi-class\n            return np.argmax(A, axis=0)\n        else:  # Binary\n            return (A &gt; 0.5).astype(int).ravel()\n\n    def accuracy(self, X, Y):\n        \"\"\"\n        Calculate accuracy\n        \"\"\"\n        Y_pred = self.predict(X)\n\n        if self.layer_dims[-1] &gt; 1:  # Multi-class\n            Y_true = np.argmax(Y, axis=1) if Y.shape[1] &gt; 1 else Y\n        else:  # Binary\n            Y_true = Y.flatten()\n\n        return np.mean(Y_pred == Y_true)\n</code></pre>"},{"location":"exercises/mlp/main/#network-architecture-comparison","title":"Network Architecture Comparison","text":"<p>I experimented with different architectures to analyze performance:</p> <p>Architecture Comparison Experiment</p> <pre><code># Create dataset for network comparison\nX, y = make_classification(n_samples=1000, n_features=2, n_informative=2, \n                          n_redundant=0, n_classes=2, n_clusters_per_class=2,\n                          class_sep=1.0, random_state=42)\n\n# Split data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# One-hot encode for multi-class (even though this is binary for demonstration)\nencoder = OneHotEncoder(sparse=False)\ny_train_onehot = encoder.fit_transform(y_train.reshape(-1, 1))\ny_test_onehot = encoder.transform(y_test.reshape(-1, 1))\n\n# Define different architectures\narchitectures = [\n    [2, 4, 2],            # 1 hidden layer with 4 neurons\n    [2, 8, 2],            # 1 hidden layer with 8 neurons\n    [2, 4, 4, 2],         # 2 hidden layers with 4 neurons each\n    [2, 8, 4, 2],         # 2 hidden layers: 8 neurons, then 4\n    [2, 4, 4, 4, 2]       # 3 hidden layers with 4 neurons each\n]\n\n# Train and evaluate each architecture\nresults = []\n\nfor arch in architectures:\n    # Create and train model\n    model = DeepMLP(layer_dims=arch, learning_rate=0.1)\n    losses = model.fit(X_train, y_train_onehot, epochs=1000)\n\n    # Evaluate\n    train_acc = model.accuracy(X_train, y_train_onehot)\n    test_acc = model.accuracy(X_test, y_test_onehot)\n\n    # Store results\n    results.append({\n        'architecture': arch,\n        'train_accuracy': train_acc,\n        'test_accuracy': test_acc,\n        'final_loss': losses[-1]\n    })\n\n    print(f\"Architecture {arch}: Train Acc = {train_acc:.4f}, Test Acc = {test_acc:.4f}\")\n</code></pre> <p>Output: <pre><code>Created network with 2 layers: [2, 4, 2]\nEpoch 0, Cost: 0.693636\nEpoch 100, Cost: 0.474606\nEpoch 200, Cost: 0.331654\nEpoch 300, Cost: 0.295071\nEpoch 400, Cost: 0.278736\nEpoch 500, Cost: 0.268990\nEpoch 600, Cost: 0.262219\nEpoch 700, Cost: 0.257141\nEpoch 800, Cost: 0.253157\nEpoch 900, Cost: 0.249916\nArchitecture [2, 4, 2]: Train Acc = 0.8850, Test Acc = 0.8600\nCreated network with 2 layers: [2, 8, 2]\nEpoch 0, Cost: 0.693564\nEpoch 100, Cost: 0.456462\nEpoch 200, Cost: 0.298247\nEpoch 300, Cost: 0.266189\nEpoch 400, Cost: 0.252141\nEpoch 500, Cost: 0.244287\nEpoch 600, Cost: 0.239048\nEpoch 700, Cost: 0.235246\nEpoch 800, Cost: 0.232309\nEpoch 900, Cost: 0.229931\nArchitecture [2, 8, 2]: Train Acc = 0.9050, Test Acc = 0.8800\n\nCreated network with 3 layers: [2, 4, 4, 2]\nEpoch 0, Cost: 0.693391\nEpoch 100, Cost: 0.412886\nEpoch 200, Cost: 0.275092\nEpoch 300, Cost: 0.239888\nEpoch 400, Cost: 0.222528\nEpoch 500, Cost: 0.211878\nEpoch 600, Cost: 0.204505\nEpoch 700, Cost: 0.199002\nEpoch 800, Cost: 0.194778\nEpoch 900, Cost: 0.191438\nArchitecture [2, 4, 4, 2]: Train Acc = 0.9200, Test Acc = 0.9000\n\nCreated network with 3 layers: [2, 8, 4, 2]\nEpoch 0, Cost: 0.693316\nEpoch 100, Cost: 0.399902\nEpoch 200, Cost: 0.251841\nEpoch 300, Cost: 0.213546\nEpoch 400, Cost: 0.193541\nEpoch 500, Cost: 0.180857\nEpoch 600, Cost: 0.171797\nEpoch 700, Cost: 0.164978\nEpoch 800, Cost: 0.159676\nEpoch 900, Cost: 0.155422\nArchitecture [2, 8, 4, 2]: Train Acc = 0.9425, Test Acc = 0.9150\n\nCreated network with 4 layers: [2, 4, 4, 4, 2]\nEpoch 0, Cost: 0.693534\nEpoch 100, Cost: 0.405793\nEpoch 200, Cost: 0.267062\nEpoch 300, Cost: 0.226257\nEpoch 400, Cost: 0.204248\nEpoch 500, Cost: 0.189724\nEpoch 600, Cost: 0.179249\nEpoch 700, Cost: 0.171238\nEpoch 800, Cost: 0.164846\nEpoch 900, Cost: 0.159608\nArchitecture [2, 4, 4, 4, 2]: Train Acc = 0.9363, Test Acc = 0.9100\n</code></pre></p>"},{"location":"exercises/mlp/main/#architecture-comparison-results","title":"Architecture Comparison Results","text":"Architecture Hidden Layers Training Accuracy Testing Accuracy Final Loss [2, 4, 2] 1 layer: 4 neurons 0.8850 0.8600 0.2499 [2, 8, 2] 1 layer: 8 neurons 0.9225 0.9100 0.1705 [2, 4, 4, 2] 2 layers: 4, 4 neurons 0.9238 0.9200 0.1699 [2, 8, 4, 2] 2 layers: 8, 4 neurons 0.9425 0.9150 0.1554 [2, 4, 4, 4, 2] 3 layers: 4, 4, 4 neurons 0.9363 0.9100 0.1596 <p>The comparison shows that deeper networks generally achieve better performance, with the best architecture being [2, 8, 4, 2] which has two hidden layers with 8 and 4 neurons respectively.</p>"},{"location":"exercises/mlp/main/#decision-boundary-visualization_1","title":"Decision Boundary Visualization","text":"<p>I visualized the decision boundaries for the best-performing architecture:</p> <p>Decision Boundary Visualization</p> <pre><code># Get best architecture (based on test accuracy)\nbest_arch_idx = np.argmax([r['test_accuracy'] for r in results])\nbest_arch = architectures[best_arch_idx]\nprint(f\"Best architecture: {best_arch}\")\n\n# Train model with best architecture for visualization\nbest_model = DeepMLP(layer_dims=best_arch, learning_rate=0.1)\nbest_model.fit(X_train, y_train_onehot, epochs=1000)\n\n# Create mesh grid for visualization\nx_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\ny_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\nxx, yy = np.meshgrid(np.arange(x_min, x_max, 0.01), np.arange(y_min, y_max, 0.01))\ngrid_points = np.c_[xx.ravel(), yy.ravel()]\n\n# Get predictions\nZ = best_model.predict(grid_points)\nZ = Z.reshape(xx.shape)\n\n# Plot decision boundary\nplt.figure(figsize=(10, 8))\nplt.contourf(xx, yy, Z, alpha=0.8, cmap='coolwarm')\nplt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, edgecolors='k', cmap='coolwarm')\nplt.title(f\"Decision Boundary with Best Architecture: {best_arch}\")\nplt.xlabel(\"Feature 1\")\nplt.ylabel(\"Feature 2\")\nplt.show()\n</code></pre> <p></p>"},{"location":"exercises/mlp/main/#key-learning-outcomes_3","title":"Key Learning Outcomes","text":"<p>This exercise demonstrated several important concepts:</p> <ol> <li>Deep Network Implementation: Successfully built and trained networks with multiple hidden layers</li> <li>Architecture Comparison: Analyzed how depth and width affect model performance</li> <li>Generalization: Deeper networks generally showed better test accuracy (up to a point)</li> <li>Efficient Backpropagation: Implemented gradient propagation through multiple layers</li> <li>Capacity vs. Complexity: Explored the tradeoff between model capacity and training complexity</li> </ol> <p>The deeper MLP implementation showed that increasing network depth can improve model performance on complex datasets, but also requires careful design and tuning.</p>"},{"location":"exercises/perceptron/main/","title":"Exercise 2 - Perceptron Implementation","text":""},{"location":"exercises/perceptron/main/#overview","title":"Overview","text":"<p>This exercise explores the fundamental concepts of perceptron learning and its limitations through two distinct scenarios:</p> <ol> <li>Well-Separated Classes - Understanding perceptron convergence with linearly separable data</li> <li>Overlapping Classes - Exploring perceptron limitations with non-separable data</li> </ol> <p>The exercise demonstrates the perceptron convergence theorem in practice and highlights why more complex neural networks are needed for real-world classification problems.</p>"},{"location":"exercises/perceptron/main/#exercise-1-linearly-separable-data","title":"Exercise 1: Linearly Separable Data","text":""},{"location":"exercises/perceptron/main/#dataset-generation","title":"Dataset Generation","text":"<p>I generated a 2D dataset with two well-separated classes, each following a normal distribution:</p> <ul> <li>Class 0 (Blue): Mean = [1.5, 1.5], Covariance Matrix = [[0.5, 0], [0, 0.5]], 1000 samples</li> <li>Class 1 (Orange): Mean = [5, 5], Covariance Matrix = [[0.5, 0], [0, 0.5]], 1000 samples</li> </ul> <p>The dataset was generated using <code>numpy.random.multivariate_normal()</code> with these exact parameters to ensure reproducible results and clear class separation.</p> <p></p> <p>Figure 1: Exercise 1 dataset showing two well-separated classes with clear linear separability. Class 0 (blue) centered at [1.5, 1.5] and Class 1 (orange) centered at [5, 5].</p> <p>Dataset Generation Code</p> <pre><code>import numpy as np\nimport pandas as pd\n\n# Class 0: Mean = [1.5, 1.5], Covariance = [[0.5, 0], [0, 0.5]]\nmu = [1.5, 1.5]\ncov = [[0.5, 0], [0, 0.5]]  # Diagonal covariance\n\nclass_0 = np.random.multivariate_normal(mu, cov, 1000)\nsample['feature1'] = class_0[:, 0]\nsample['feature2'] = class_0[:, 1]\nsample['label'] = 0\n\n# Class 1: Mean = [5, 5], Covariance = [[0.5, 0], [0, 0.5]]\nmu = [5, 5]\ncov = [[0.5, 0], [0, 0.5]]  # Diagonal covariance\n\nclass_1 = np.random.multivariate_normal(mu, cov, 1000)\ntemp = pd.DataFrame()\ntemp['feature1'] = class_1[:, 0]\ntemp['feature2'] = class_1[:, 1]\ntemp['label'] = 1\nsample = pd.concat([sample, temp], ignore_index=True)\n</code></pre>"},{"location":"exercises/perceptron/main/#perceptron-implementation-from-scratch","title":"Perceptron Implementation from Scratch","text":"<p>Following the exercise requirements, I implemented a single-layer perceptron using only NumPy for basic linear algebra operations:</p> <p>Perceptron Implementation</p> <pre><code>class Perceptron:\n    def __init__(self, learning_rate=0.01, n_iters=100):\n        self.learning_rate = learning_rate\n        self.n_iters = n_iters\n        self.weights = None\n        self.bias = None\n        self.accuracies = []\n\n    def fit(self, X, y):\n        # Initialize weights and bias\n        n_features = X.shape[1]\n        self.weights = np.zeros(n_features)\n        self.bias = 0\n\n        # Convert labels to -1 and 1 for perceptron algorithm\n        y_train = np.where(y == 0, -1, 1)\n\n        # Training loop\n        for epoch in range(self.n_iters):\n            errors = 0\n\n            for i in range(X.shape[0]):\n                # Calculate prediction\n                linear_output = np.dot(X[i], self.weights) + self.bias\n                y_pred = 1 if linear_output &gt;= 0 else -1\n\n                # Update weights if misclassified\n                if y_train[i] != y_pred:\n                    self.weights += self.learning_rate * y_train[i] * X[i]\n                    self.bias += self.learning_rate * y_train[i]\n                    errors += 1\n\n            # Calculate accuracy for this epoch\n            accuracy = self.score(X, y)\n            self.accuracies.append(accuracy)\n\n            # Check for convergence\n            if errors == 0:\n                print(f\"Converged at epoch {epoch + 1}\")\n                break\n\n    def predict(self, X):\n        linear_output = np.dot(X, self.weights) + self.bias\n        return np.where(linear_output &gt;= 0, 1, 0)\n\n    def score(self, X, y):\n        predictions = self.predict(X)\n        return np.mean(predictions == y)\n</code></pre>"},{"location":"exercises/perceptron/main/#training-results","title":"Training Results","text":""},{"location":"exercises/perceptron/main/#the-perceptron-was-trained-with-the-following-parameters","title":"The perceptron was trained with the following parameters:","text":"<ul> <li>Learning Rate: \u03b7 = 0.01</li> <li>Maximum Epochs: 100</li> <li>Convergence Criterion: No weight updates in a full pass over the dataset</li> </ul>"},{"location":"exercises/perceptron/main/#training-performance","title":"Training Performance","text":"<ul> <li>Convergence: Achieved at epoch 12</li> <li>Final Accuracy: 100.00% (2000/2000 samples correctly classified)</li> <li>Final Weights: [0.0199, 0.0171]</li> <li>Final Bias: -0.1200</li> <li>Misclassified Points: 0</li> </ul>"},{"location":"exercises/perceptron/main/#visualization-and-analysis","title":"Visualization and Analysis","text":"<p>The training progress and decision boundary visualization demonstrate the perceptron's effectiveness on linearly separable data:</p> <p></p> <p>Figure 2: Training accuracy progression for Exercise 1, showing rapid convergence to 100% accuracy in just 12 epochs.</p> <p>Training Accuracy Plot Code</p> <pre><code># Plot accuracies\nplt.plot(range(1, len(perceptron.accuracies) + 1), perceptron.accuracies, marker='o')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.title('Perceptron Training Accuracy over Epochs')\nplt.show()\n</code></pre> <p>Training Accuracy Progress</p> <p>The perceptron achieved rapid convergence, reaching 100% accuracy in just 12 epochs. The accuracy curve shows a steady improvement from the initial random state to perfect classification.</p> <p></p> <p>Figure 3: Decision boundary visualization for Exercise 1. The linear boundary perfectly separates the two classes with a clear margin, demonstrating successful linearly separable classification.</p> <p>Decision Boundary Plot Code</p> <pre><code># Decision boundary visualization\nh = 0.02  # step size in the mesh\n\nx_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\ny_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                     np.arange(y_min, y_max, h))\nZ = perceptron.predict(np.c_[xx.ravel(), yy.ravel()])\nZ = Z.reshape(xx.shape)\n\nplt.contourf(xx, yy, Z, alpha=0.2, cmap='Set1')\nplt.title('Perceptron Decision Boundary')\nplt.xlabel('Feature 1')\nplt.ylabel('Feature 2')\nsns.scatterplot(data=sample, x='feature1', y='feature2', hue='label', palette='Set1', edgecolor='k')\nplt.show()\n</code></pre> <p>Decision Boundary Analysis</p> <p>The learned decision boundary (defined by w\u00b7x + b = 0) perfectly separates the two classes with a clear margin. The boundary equation is: <pre><code>0.0199 * feature1 + 0.0171 * feature2 - 0.1200 = 0\n</code></pre></p> <p>This can be simplified to: <code>1.163 * feature1 + feature2 = 7.018</code>, which creates an effective separation between the class means at [1.5, 1.5] and [5, 5].</p>"},{"location":"exercises/perceptron/main/#why-linearly-separable-data-leads-to-quick-convergence","title":"Why Linearly Separable Data Leads to Quick Convergence","text":"<p>The rapid convergence observed in Exercise 1 can be understood by examining how the perceptron learning process works:</p>"},{"location":"exercises/perceptron/main/#why-the-perceptron-succeeded","title":"Why the Perceptron Succeeded","text":"<ul> <li>Clear Separation: The two classes are far apart with no overlap, making it easy to draw a line between them</li> <li>Consistent Learning: Every mistake the perceptron makes provides clear guidance on which direction to adjust the decision line</li> <li>Stable Solution: Once the perceptron finds a good separating line, no more adjustments are needed</li> </ul>"},{"location":"exercises/perceptron/main/#key-factors-that-made-learning-fast","title":"Key Factors That Made Learning Fast","text":"<ol> <li>Well-Separated Classes: The 3.5-unit distance between class centers (1.5,1.5) and (5,5) creates plenty of space for a decision boundary</li> <li>Tight Data Clusters: Low variance (0.5) keeps most data points close to their class centers, reducing edge cases</li> <li>No Conflicting Examples: Since classes don't overlap, the perceptron never encounters contradictory training signals</li> </ol>"},{"location":"exercises/perceptron/main/#exercise-2-overlapping-classes","title":"Exercise 2: Overlapping Classes","text":""},{"location":"exercises/perceptron/main/#dataset-generation_1","title":"Dataset Generation","text":"<p>For Exercise 2, I generated a more challenging dataset with overlapping classes:</p> <ul> <li>Class 0 (Blue): Mean = [3, 3], Covariance Matrix = [[1.5, 0], [0, 1.5]], 1000 samples</li> <li>Class 1 (Orange): Mean = [4, 4], Covariance Matrix = [[1.5, 0], [0, 1.5]], 1000 samples</li> </ul> <p>This configuration creates significant class overlap due to: - Closer Means: Only 1.41 units apart (\u221a2 distance) compared to 4.95 units in Exercise 1 - Higher Variance: 1.5 variance creates wider distributions with more spread</p> <p></p> <p>Figure 4: Exercise 2 dataset showing overlapping classes with significant intersection between Class 0 (blue) and Class 1 (orange), making linear separation impossible.</p> <p>Dataset Visualization Code</p> <pre><code># Plot the overlapping dataset for Exercise 2\nplt.figure(figsize=(8, 6))\nsns.scatterplot(data=sample[0], x='feature1', y='feature2', hue='label', palette='Set1')\nplt.title('Exercise 2: Overlapping Classes Dataset')\nplt.xlabel('Feature 1')\nplt.ylabel('Feature 2')\nplt.show()\n</code></pre> <p>Dataset Characteristics</p> <pre><code># Class 0: Mean = [3, 3], Covariance = [[1.5, 0], [0, 1.5]]\nmu = [3, 3]\ncov = [[1.5, 0], [0, 1.5]]\nclass_0_run = np.random.multivariate_normal(mu, cov, 1000)\n\n# Class 1: Mean = [4, 4], Covariance = [[1.5, 0], [0, 1.5]]\nmu = [4, 4]\ncov = [[1.5, 0], [0, 1.5]]\nclass_1_run = np.random.multivariate_normal(mu, cov, 1000)\n</code></pre>"},{"location":"exercises/perceptron/main/#training-results-on-overlapping-data","title":"Training Results on Overlapping Data","text":"<p>Using the same perceptron implementation and training parameters as Exercise 1:</p>"},{"location":"exercises/perceptron/main/#training-performance_1","title":"Training Performance","text":"<ul> <li>Convergence: Failed to converge within 100 epochs</li> <li>Final Accuracy: 50.25% (1005/2000 samples correctly classified)</li> <li>Final Weights: [0.42, 0.42]</li> <li>Final Bias: -0.58</li> <li>Misclassified Points: 995 (49.75% error rate)</li> </ul>"},{"location":"exercises/perceptron/main/#multiple-runs-analysis","title":"Multiple Runs Analysis","text":"<p>To assess consistency, I ran the experiment 5 times with different random seeds:</p> Run Accuracy Epochs Converged 1 50.90% 100 No 2 51.10% 100 No 3 50.85% 100 No 4 50.05% 100 No 5 50.25% 100 No <p>Summary: Average accuracy = 50.63% \u00b1 0.41%, showing consistent failure across all runs.</p>"},{"location":"exercises/perceptron/main/#visualization-of-overlapping-classes","title":"Visualization of Overlapping Classes","text":"<p>Figure 5: Training accuracy for Exercise 2 showing oscillating behavior and failure to converge, with accuracy hovering around 50% throughout 100 epochs.</p> <p>Training Accuracy Plot Code</p> <pre><code># Plot accuracies\nplt.plot(range(1, len(perceptron.accuracies) + 1), perceptron.accuracies, marker='o')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.title('Perceptron Training Accuracy over Epochs')\nplt.show()\n</code></pre> <p></p> <p>Decision Boundary Visualization Code</p> <pre><code># Highlight misclassified points\npredictions = perceptron.predict(X)\nmisclassified_mask = predictions != y\ncorrectly_classified_mask = predictions == y\n\nplt.figure(figsize=(10, 8))\n# Plot correctly classified points\nscatter1 = plt.scatter(X[correctly_classified_mask, 0], X[correctly_classified_mask, 1], \n           c=y[correctly_classified_mask], cmap='Set1', alpha=0.7, s=50, \n           label='Correctly Classified', edgecolors='black', linewidth=0.5)\n\n# Plot misclassified points with different markers\nplt.scatter(X[misclassified_mask, 0], X[misclassified_mask, 1], \n           c=y[misclassified_mask], cmap='Set1', alpha=0.9, s=100, \n           marker='x', linewidth=3, label='Misclassified')\n\n# Plot decision boundary\nh = 0.02\nx_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\ny_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                     np.arange(y_min, y_max, h))\nZ = perceptron.predict(np.c_[xx.ravel(), yy.ravel()])\nZ = Z.reshape(xx.shape)\nplt.contour(xx, yy, Z, levels=[0.5], colors='red', linestyles='--', linewidths=2)\n\nplt.title('Exercise 2: Decision Boundary with Misclassified Points Highlighted')\nplt.xlabel('Feature 1')\nplt.ylabel('Feature 2')\nplt.legend()\nplt.show()\n</code></pre> <p>Figure 6: Decision boundary for Exercise 2 with misclassified points highlighted. The linear boundary cannot properly separate the overlapping classes, resulting in numerous misclassifications (shown as X markers).</p> <p>The decision boundary visualization reveals the fundamental limitation of linear classifiers on non-separable data:</p> <p>Decision Boundary Limitations</p> <p>The perceptron's linear decision boundary cuts through the overlapping region, inevitably misclassifying samples from both classes. The boundary oscillates during training as it encounters contradictory examples in the overlap zone.</p>"},{"location":"exercises/perceptron/main/#comparative-analysis-exercise-1-vs-exercise-2","title":"Comparative Analysis: Exercise 1 vs Exercise 2","text":"Aspect Exercise 1 (Separable) Exercise 2 (Overlapping) Class Distance 4.95 units 1.41 units Variance 0.5 (low spread) 1.5 (high spread) Convergence 12 epochs Never (100+ epochs) Final Accuracy 100.00% 50.63% \u00b1 0.41% Decision Boundary Stable, optimal Oscillating, suboptimal Misclassifications 0 points ~988 points <p>Figure 7: Side-by-side comparison of the two datasets, clearly showing the difference between linearly separable (left) and overlapping (right) classes.</p> <p></p> <p>Figure 8: Training accuracy comparison between Exercise 1 (blue) and Exercise 2 (orange), demonstrating the stark difference in convergence behavior between separable and non-separable data.</p>"},{"location":"exercises/perceptron/main/#why-exercise-2-failed-to-converge","title":"Why Exercise 2 Failed to Converge","text":""},{"location":"exercises/perceptron/main/#mathematical-impossibility","title":"Mathematical Impossibility","text":"<p>The failure to converge in Exercise 2 stems from a fundamental mathematical limitation:</p> <ol> <li>No Linear Separator Exists: The overlapping distributions cannot be perfectly separated by any straight line</li> <li>Contradictory Updates: The perceptron encounters points from different classes in the same region, leading to conflicting weight updates</li> <li>Perpetual Oscillation: The algorithm continuously adjusts weights in response to misclassifications, never reaching a stable state</li> </ol>"},{"location":"exercises/perceptron/main/#the-perceptron-limitation","title":"The Perceptron Limitation","text":"<p>This exercise perfectly demonstrates the Perceptron Limitation Theorem: perceptrons can only solve linearly separable problems. When data is not linearly separable:</p> <ul> <li>The algorithm will never converge</li> <li>Accuracy will remain poor (often near random guessing)</li> <li>The decision boundary will oscillate indefinitely</li> </ul>"},{"location":"exercises/perceptron/main/#implications-for-neural-network-design","title":"Implications for Neural Network Design","text":""},{"location":"exercises/perceptron/main/#why-multi-layer-networks-are-essential","title":"Why Multi-Layer Networks Are Essential","text":"<p>The results from Exercise 2 highlight why modern neural networks use multiple layers:</p> <ol> <li>Non-Linear Transformations: Hidden layers can transform the input space to make classes separable</li> <li>Feature Learning: Multiple layers can learn complex feature combinations that reveal hidden patterns</li> <li>Universal Approximation: Deep networks can approximate any continuous function, handling arbitrary decision boundaries</li> </ol>"},{"location":"projects/classification/main/","title":"Project 1 - Classification","text":"<p>Aqui vai toda a documenta\u00e7\u00e3o do projeto, incluindo o que j\u00e1 foi feito e o que falta fazer.</p>"},{"location":"thisdocumentation/main/","title":"Documentation","text":""},{"location":"thisdocumentation/main/#pre-requisitos","title":"Pr\u00e9-requisitos","text":"<p>Antes de come\u00e7ar, certifique-se de que voc\u00ea possui os seguintes pr\u00e9-requisitos instalados em seu sistema:</p> <ul> <li>Git: Para clonar o reposit\u00f3rio.</li> </ul>"},{"location":"thisdocumentation/main/#instalando-o-python","title":"Instalando o Python","text":"LinuxmacOSWindows <p>Instale o Python 3.8 ou superior.</p> <pre><code>sudo apt install python3 python3-venv python3-pip\npython3 --version\n</code></pre> <p>Instale o Python 3.8 ou superior.</p> <pre><code>brew install python\npython3 --version\n</code></pre> <p>Instale o Python 3.13 ou superior. Baixe o instalador do site oficial do Python (https://www.python.org/downloads/) e execute-o. Certifique-se de marcar a op\u00e7\u00e3o \"Add Python to PATH\" durante a instala\u00e7\u00e3o.</p> <pre><code>python --version\n</code></pre>"},{"location":"thisdocumentation/main/#usage","title":"Usage","text":"<p>Para utilizar o c\u00f3digo deste reposit\u00f3rio, siga as instru\u00e7\u00f5es a seguir:</p> <p>Clone ou fork este reposit\u00f3rio:</p> <pre><code>git clone &lt;URL_DO_REPOSITORIO&gt;\n</code></pre> <p>Crie um ambiente virtual do Python:</p> Linux/macOSWindows <pre><code>python3 -m venv env\n</code></pre> <pre><code>python -m venv env\n</code></pre> <p>Ative o ambiente virtual (voc\u00ea deve fazer isso sempre que for executar algum script deste reposit\u00f3rio):</p> Linux/macOSWindows <pre><code>source ./env/bin/activate\n</code></pre> <pre><code>.\\env\\Scripts\\activate\n</code></pre> <p>Instale as depend\u00eancias com:</p> Linux/macOSWindows <pre><code>python3 -m pip install -r requirements.txt --upgrade\n</code></pre> <pre><code>python -m pip install -r requirements.txt --upgrade\n</code></pre>"},{"location":"thisdocumentation/main/#deployment","title":"Deployment","text":"<p>O material utiliza o mkdocs para gerar a documenta\u00e7\u00e3o. Para visualizar a documenta\u00e7\u00e3o, execute o comando:</p> <pre><code>mkdocs serve -o\n</code></pre> <p>Para subir ao GitHub Pages, execute o comando:</p> <pre><code>mkdocs gh-deploy\n</code></pre> <p>Esse reposit\u00f3rio possui um workflow do GitHub Actions que executa o comando <code>mkdocs gh-deploy</code> sempre que houver um push na branch <code>main</code>. Assim, n\u00e3o \u00e9 necess\u00e1rio executar esse comando manualmente. Toda vez que voc\u00ea fizer um push na branch <code>main</code>, a documenta\u00e7\u00e3o ser\u00e1 atualizada automaticamente no GitHub Pages.</p> <p>Aviso 1</p> <p>Para que o github actions funcione corretamente, \u00e9 necess\u00e1rio que o reposit\u00f3rio esteja configurado para que o bot <code>github-actions[bot]</code> tenha permiss\u00e3o de escrita. Voc\u00ea pode verificar isso nas configura\u00e7\u00f5es do reposit\u00f3rio, na se\u00e7\u00e3o \"Actions\" e depois em \"General\". Certifique-se de que a op\u00e7\u00e3o \"Workflow permissions\" esteja definida como \"Read and write permissions\".</p> <p></p> <p>Aviso 2</p> <p>Depois de publicar, caso n\u00e3o consiga acessar a p\u00e1gina, verifique se o github pages est\u00e1 configurado corretamente. V\u00e1 at\u00e9 as configura\u00e7\u00f5es do reposit\u00f3rio, na se\u00e7\u00e3o \"Pages\" e verifique se a branch <code>gh-pages</code> est\u00e1 selecionada como fonte. Se n\u00e3o estiver, selecione-a e salve as altera\u00e7\u00f5es.</p> <p></p> <p>Pay Attention</p> <p>No arquivo '<code>mkdocs.yml</code>, a se\u00e7\u00e3o <code>site_url</code> deve estar configurada corretamente para o seu reposit\u00f3rio. Por exemplo, se o seu reposit\u00f3rio estiver em <code>https://github.com/usuario/repositorio</code>, a se\u00e7\u00e3o <code>site_url</code> deve ser:</p> <pre><code>site_url: https://usuario.github.io/repositorio\n</code></pre> <p>Tamb\u00e9m, certifique-se de que a se\u00e7\u00e3o <code>repo_url</code> esteja configurada corretamente para o seu reposit\u00f3rio. Por exemplo:</p> <pre><code>repo_url: https://github.com/usuario/repositorio\n</code></pre>"}]}